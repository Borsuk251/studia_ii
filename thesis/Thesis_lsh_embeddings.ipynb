{"cells":[{"cell_type":"code","execution_count":null,"source":["import morfeusz2\n","morf = morfeusz2.Morfeusz()\n","import pandas as pd\n","import numpy as np\n","import re\n","\n","import string\n","from string import ascii_lowercase\n","from sklearn.metrics import f1_score, confusion_matrix\n","\n","import gensim\n","from gensim.models import Word2Vec\n","from gensim.models import FastText\n","import unicodedata\n","!pip install emoji\n","import emoji\n","from string import punctuation\n","import pickle"],"outputs":[{"output_type":"stream","name":"stdout","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: emoji in /home/oskar/.local/lib/python3.9/site-packages (1.6.3)\n"]}],"metadata":{"id":"fAbkgRp_ycYC","outputId":"dc47b28a-dbe7-4037-ed21-145c0cf3d89e"}},{"cell_type":"code","source":["!pip install gdown\n","\n","!gdown --id 1kbYmGkMsQGVBdnDStxx3nz79WNh5JXy7 # training_set_clean_only_text.txt\n","# !gdown --id 1XP30I7gKxhY1jOCOFlcIWDQDnZ3Qk9rZ # training_set_clean_only_tags.txt\n","# !gdown --id 1z0Laz8jCGR-GQcMNL2_qKsGnbL4VRRCY # test_set_clean_only_text.txt\n","# !gdown --id 1o_XRCnKlScCrZRk5suzjwzljT7YonQ2q # test_set_clean_only_tags.txt\n","\n","\n","!gdown --id 1gJL8GYTuWj5BXUUFnCy--AE4xEZlDr1u # corpora\n","\n","!gdown --id 1-IeRYiTy5rujiTg97jrEzhCSpdJkoBL1 # dataset4\n","\n","!gdown --id 1IezKpxY0c8OHv0nq44yrUH8pYtHnsnY- # dataset5\n"],"metadata":{"id":"ee064vxSSkYS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LSH with simpler processing"],"metadata":{"id":"USkTjU7vyr1H"}},{"cell_type":"code","execution_count":null,"source":["print(gensim.__version__)"],"outputs":[{"output_type":"stream","name":"stdout","text":["3.6.0\n"]}],"metadata":{"id":"Rio1KcPXycYH","outputId":"d3432c61-7ff4-454e-ab23-82964bdf9690"}},{"cell_type":"code","execution_count":null,"source":["dataset_clean = []\n","with open('dataset_clean4.txt', 'r') as file2:\n","  for i, line in enumerate(file2):\n","    dataset_clean.append(line.split())"],"outputs":[],"metadata":{"id":"QarPzoxgycYH"}},{"cell_type":"code","execution_count":null,"source":["def read_file(file_name): # read given file and create list of phrases\n","  lines = []\n","  with open(file_name, encoding=\"UTF-8-SIG\") as text:\n","    for line in text:\n","      lines.append(line[:-1])\n","  return lines\n","  \n","text = read_file('training_set_clean_only_text.txt')"],"outputs":[],"metadata":{"id":"e5a4ZG-3ycYI"}},{"cell_type":"code","execution_count":null,"source":["text[:10]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Dla mnie faworytem do tytuu bdzie Cracovia. Zobaczymy, czy typ si sprawdzi.',\n"," '@anonymized_account @anonymized_account Brawo ty Daria kibic ma by na dobre i ze',\n"," '@anonymized_account @anonymized_account Super, polski premier skada kwiaty na grobach kolaborant贸w. Ale doczekalimy czas贸w.',\n"," '@anonymized_account @anonymized_account Musi. Innej drogi nie mamy.',\n"," 'Odrzut natychmiastowy, kwana mina, mam problem',\n"," 'Jaki on by fajny xdd pamitam, 偶e sp贸藕niam si na jego pierwsze zajcia i to sporo i za kar kaza mi usi w pierwszej awce XD',\n"," '@anonymized_account No nie ma u nas szczcia ',\n"," '@anonymized_account Dawno kogo tak wrednego nie widziaam xd',\n"," '@anonymized_account @anonymized_account Zalegoci byy, ale wa偶ne czy byy wezwania do zapaty z kt贸rych si klub nie wywiza.',\n"," '@anonymized_account @anonymized_account @anonymized_account Gdzie jest @anonymized_account . Brudziski jeste kamc i marnym kutasem @anonymized_account']"]},"metadata":{},"execution_count":6}],"metadata":{"id":"4nrVhbqGycYI","outputId":"25f39730-8347-470c-92ed-00b5e8d272b6"}},{"cell_type":"code","execution_count":null,"source":["not_wanted_words = ['@anonymized_account']\n","\n","def word_process(word):\n","  if word in not_wanted_words:\n","    return False\n","\n","  emoji_pattern = re.compile(\"[\"\n","          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                            \"]+\", flags=re.UNICODE)\n","  word = emoji_pattern.sub(r'', word) # no emoji\n","  word = word.lower()\n","  analysis = morf.analyse(word)\n","  for interpretation in analysis:\n","    word = interpretation[2][1].split(':')[0]\n","    break \n","  word = word.lower()\n","  if len(word)==0:\n","    return False\n","  return word\n","\n","def clean_dataset(dataset):\n","  dataset_clean = []\n","  for line in dataset:\n","    row = []\n","    words = line.split()\n","    if words[0] == 'RT': \n","      continue\n","    for word in words:\n","      word = word_process(word)\n","      if word != False: \n","        row.append(word)\n","    dataset_clean.append(' '.join(row))\n","  return dataset_clean\n","\n","clean_train = clean_dataset(text)"],"outputs":[],"metadata":{"id":"pGQpy4iYycYJ"}},{"cell_type":"code","execution_count":null,"source":["clean_train[:10]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["['dla ja faworyt do tytu by cracovia zobaczy czy typ si sprawdzi',\n"," 'brawo ty daria kibic mie by na dobry i ze',\n"," 'super polski premier skada kwiat na gr贸b kolaborant ale doczeka czas',\n"," 'muszy inny drogi nie mama',\n"," 'odrzut natychmiastowy kwany mina mama problem',\n"," 'jaki on by fajny xdd pamita 偶e sp贸藕ni si na on pierwsze zajcia i to sporo i za kara kaza ksiga_micheasza usi w pierwszy awka xd',\n"," 'no nie mie u my szczcie',\n"," 'dawno kto taka wredny nie widzie xd',\n"," 'zalego byy ale wa偶ny czy byy wezwanie do zapata z kt贸ry si klub nie wywiza',\n"," 'gdzie by . brudziski by kamca i marny kutas']"]},"metadata":{},"execution_count":5}],"metadata":{"id":"eotntFlUycYK","outputId":"59c6d5c1-f59d-41b0-908a-b5ac061d82b1"}},{"cell_type":"code","execution_count":null,"source":["for sen in clean_train:\n","  dataset_clean.append(sen.split())\n","len(dataset_clean)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["5009385"]},"metadata":{},"execution_count":6}],"metadata":{"id":"VWXgm244ycYL","outputId":"06d629fc-c93d-4a5b-9d88-bb4ec435bdcf"}},{"cell_type":"code","execution_count":null,"source":["def cossim(u,v):\n","  if np.all(u == v):\n","    return 1\n","  norm = np.linalg.norm(u)*np.linalg.norm(v)\n","  cosine = u@v/norm\n","  ang = np.arccos(cosine)\n","  return 1-ang/np.pi"],"outputs":[],"metadata":{"id":"FoSGx2ivycYL"}},{"cell_type":"code","execution_count":null,"source":["model_embeddings128_fast_with_train = gensim.models.Word2Vec.load(\"word2vec_fast_128_with_train.model\")"],"outputs":[],"metadata":{"id":"N5m04l99ycYM"}},{"cell_type":"code","execution_count":null,"source":["cossim(model_embeddings128_fast_with_train['kobieta'], model_embeddings128_fast_with_train['1'])"],"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipykernel_42608/158147224.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  cossim(model_embeddings128_fast_with_train['kobieta'], model_embeddings128_fast_with_train['1'])\n"]},{"output_type":"execute_result","data":{"text/plain":["0.5876831846296927"]},"metadata":{},"execution_count":8}],"metadata":{"id":"y1GO2WFfycYM","outputId":"76e60f95-7919-49f6-892b-4b3fd25e12db"}},{"cell_type":"code","execution_count":null,"source":["def create_trainings_dict(train_data):\n","  training_dict = dict()\n","  for sen in train_data:\n","    sen = sen.split()\n","    for word in sen:\n","      if word in training_dict:\n","        training_dict[word] +=1 \n","      else:\n","        training_dict[word] = 1\n","\n","  training_word_min_count2 = []\n","  for word in training_dict:\n","    if training_dict[word] >= 2:\n","      training_word_min_count2.append(word)\n","  return training_dict, training_word_min_count2\n","\n","training_dict, training_word_min_count2 = create_trainings_dict(clean_train)"],"outputs":[],"metadata":{"id":"5gNuJxu_ycYM"}},{"cell_type":"code","execution_count":null,"source":["len(training_dict)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["13715"]},"metadata":{},"execution_count":10}],"metadata":{"id":"xY1BWVfJycYN","outputId":"62e170b6-8ad0-4a78-9690-88a1f0edfe5a"}},{"cell_type":"code","execution_count":null,"source":["len(training_word_min_count2)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["5934"]},"metadata":{},"execution_count":11}],"metadata":{"id":"Kp7GgXgeycYN","outputId":"0e56a2cb-5978-4c5a-99fa-553be02da972"}},{"cell_type":"markdown","source":["### train 512 features"],"metadata":{"id":"en9zZ3U5zEAJ"}},{"cell_type":"code","execution_count":null,"source":["def create_lsh(embeddings, training_dict_words, feature_size=128, threshold=0.61):\n","  chosen_words = np.random.choice(np.arange(len(training_dict_words)), feature_size)\n","  embeddings_dict_size = len(embeddings.wv.vocab)\n","  lsh_dict = {}\n","  for word in embeddings.wv.vocab:\n","    lsh_dict[word] = np.zeros((feature_size,), dtype=np.short)\n","\n","  feature_dim = 0\n","  words_in_embeddings = set()\n","  for word in embeddings.wv.vocab:\n","    words_in_embeddings.add(word)\n","  for chosen_word_id in chosen_words:\n","\n","    print(feature_dim)\n","    curr_word = training_dict_words[chosen_word_id]\n","    \n","    for word in words_in_embeddings:\n","      sim_value = cossim(embeddings[word], embeddings[curr_word])\n","\n","      if sim_value >= threshold:\n","        lsh_dict[word][feature_dim] = 1\n","      else:\n","        lsh_dict[word][feature_dim] = 0\n","    feature_dim += 1\n","  return lsh_dict, chosen_words\n","\n","lsh_dict1, chosen_words1 = create_lsh(model_embeddings128_fast_with_train, training_word_min_count2, feature_size=512)\n"],"outputs":[],"metadata":{"id":"VQjXKTnXycYW"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict1_file = open(\"lsh_dict1.pickle\", \"wb\")\n","pickle.dump(lsh_dict1, lsh_dict1_file)\n","lsh_dict1_file.close()"],"outputs":[],"metadata":{"id":"GDkd7FcIycYY"}},{"cell_type":"code","execution_count":null,"source":["with open('chosen_words1.npy', 'wb') as f:\n","    np.save(f, chosen_words1)"],"outputs":[],"metadata":{"id":"2-4eZIGkycYY"}},{"cell_type":"code","execution_count":null,"source":["chosen_words1 = np.load('chosen_words1.npy',allow_pickle=True)"],"outputs":[],"metadata":{"id":"L1NBxvTeycYY"}},{"cell_type":"code","execution_count":null,"source":["len(training_word_min_count2)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["5934"]},"metadata":{},"execution_count":11}],"metadata":{"id":"Cspe0i9SycYY","outputId":"2b77f9d0-9bae-4f21-e66e-e62c13105119"}},{"cell_type":"code","execution_count":null,"source":["chosen_words1[:10]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4837, 1057,  150,  593, 5403, 3743, 1590, 2511, 3111,  744])"]},"metadata":{},"execution_count":10}],"metadata":{"id":"TX7o9TjrycYZ","outputId":"078ee978-4ffc-4cd8-c139-4604058e7b2d"}},{"cell_type":"markdown","source":["### add next 512 features"],"metadata":{"id":"u562AD3nycYZ"}},{"cell_type":"code","execution_count":null,"source":["np.random.choice(list(set(np.arange(len(training_word_min_count2))) - set(chosen_words1)), 10)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 192, 5414, 3448, 3211,  716, 1863,  310, 3123, 2053, 5292])"]},"metadata":{},"execution_count":12}],"metadata":{"id":"dIYzuEXjycYZ","outputId":"2ddbf5d7-96f4-4986-bd26-cda231478fb3"}},{"cell_type":"code","execution_count":null,"source":["def create_lsh_no_repeat(embeddings, training_dict_words, already_chosen, feature_size=128, threshold=0.61):\n","  chosen_words = np.random.choice(list(set(np.arange(len(training_dict_words))) - set(already_chosen)), feature_size)\n","  lsh_dict = {}\n","  for word in embeddings.wv.vocab:\n","    lsh_dict[word] = np.zeros((feature_size,), dtype=np.short)\n","  feature_dim = 0\n","  words_in_embeddings = set()\n","  for word in embeddings.wv.vocab:\n","    words_in_embeddings.add(word)\n","  for chosen_word_id in chosen_words:\n","    print(feature_dim)\n","    curr_word = training_dict_words[chosen_word_id]\n","    for word in words_in_embeddings:\n","      sim_value = cossim(embeddings[word], embeddings[curr_word])\n","      if sim_value >= threshold:\n","        lsh_dict[word][feature_dim] = 1\n","      else:\n","        lsh_dict[word][feature_dim] = 0\n","    feature_dim += 1\n","  return lsh_dict, chosen_words\n","\n","lsh_dict3, chosen_words3 = create_lsh_no_repeat(model_embeddings128_fast_with_train, training_word_min_count2, chosen_words1, feature_size=512)\n"],"outputs":[],"metadata":{"id":"htfTzSXrycYZ"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict3_file = open(\"lsh_dict3.pickle\", \"wb\")\n","pickle.dump(lsh_dict3, lsh_dict3_file)\n","lsh_dict3_file.close()"],"outputs":[],"metadata":{"id":"HN8bWf-wycYa"}},{"cell_type":"code","execution_count":null,"source":["with open('chosen_words3.npy', 'wb') as f:\n","    np.save(f, chosen_words3)"],"outputs":[],"metadata":{"id":"bqJR564RycYb"}},{"cell_type":"code","execution_count":null,"source":["np.concatenate((lsh_dict3['kobieta'], lsh_dict3['kobieta'])).shape"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1024,)"]},"metadata":{},"execution_count":22}],"metadata":{"id":"43fB8jNnycYb","outputId":"e3571ffb-9137-44d0-c1af-38199e639047"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict1['kobieta']"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n","       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n","       0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n","       0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n","       1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n","       1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n","       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n","       0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n","       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n","       0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n","       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n","       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n","       1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n","       0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n","       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n","       0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n","       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n","       1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","       1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n","       0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n","       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n","       0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n","       1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n","       0, 0, 1, 0, 1, 0], dtype=int16)"]},"metadata":{},"execution_count":14}],"metadata":{"id":"dvRg9mQbycYb","outputId":"d288ac02-4dd6-4a47-bd82-f1ce489bd186"}},{"cell_type":"markdown","source":["### run with vector difference as seperation"],"metadata":{"id":"m1D42qk_ycYc"}},{"cell_type":"code","execution_count":null,"source":["def create_lsh2(embeddings, training_dict_words, feature_size=128, threshold=0.61): # with difference in mind\n","  chosen_words = []\n","  lsh_dict = {}\n","  for word in embeddings.wv.vocab:\n","    lsh_dict[word] = np.zeros((feature_size,), dtype=np.short)\n","\n","  words_in_embeddings = set()\n","  for word in embeddings.wv.vocab:\n","    words_in_embeddings.add(word)\n","  for feature_dim in range(feature_size):\n","\n","    chosen_words_curr = np.random.choice(np.arange(len(training_word_min_count2)), 2)\n","    print(chosen_words_curr)\n","    curr_word1 = training_dict_words[chosen_words_curr[0]]\n","    curr_word2 = training_dict_words[chosen_words_curr[1]]\n","\n","    while len(curr_word1) < 2 or len(curr_word2) < 2:\n","      chosen_words_curr = np.random.choice(np.arange(len(training_word_min_count2)), 2)\n","      print(chosen_words_curr)\n","      curr_word1 = training_dict_words[chosen_words_curr[0]]\n","      curr_word2 = training_dict_words[chosen_words_curr[1]]\n","    chosen_words.append([curr_word1, curr_word2])\n","    cur_vec_sub = embeddings[curr_word1] - embeddings[curr_word2]\n","        print(feature_dim)\n","    \n","    for word in words_in_embeddings:\n","      sim_value = cossim(cur_vec_sub, embeddings[word])\n","\n","      if sim_value >= threshold:\n","        lsh_dict[word][feature_dim] = 1\n","      else:\n","        lsh_dict[word][feature_dim] = 0\n","  return lsh_dict, chosen_words\n","\n","lsh_dict2, chosen_words2 = create_lsh2(model_embeddings128_fast_with_train, training_word_min_count2, feature_size=512, threshold=0.47)\n","\n"],"outputs":[],"metadata":{"id":"T0FGkm3tycYc"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict2_file = open(\"lsh_dict2.pickle\", \"wb\")\n","pickle.dump(lsh_dict2, lsh_dict2_file)\n","lsh_dict2_file.close()"],"outputs":[],"metadata":{"id":"7tsXWWSDycYc"}},{"cell_type":"code","execution_count":null,"source":["with open('chosen_words2.npy', 'wb') as f:\n","    np.save(f, chosen_words2)"],"outputs":[],"metadata":{"id":"O5dzSbSoycYd"}},{"cell_type":"code","execution_count":null,"source":[""],"outputs":[],"metadata":{"id":"vb0OdLPQycYe"}},{"cell_type":"markdown","source":["## create vector of 3 values -1 0 1"],"metadata":{"id":"1xrO7ye9ycYe"}},{"cell_type":"code","execution_count":null,"source":["def create_lsh_3value(embeddings, training_dict_words, feature_size=128):\n","  chosen_words = np.random.choice(np.arange(len(training_dict_words)), feature_size)\n","  lsh_dict = {}\n","  for word in embeddings.wv.vocab:\n","    lsh_dict[word] = np.zeros((feature_size,), dtype=np.short)\n","\n","  sim_array = []\n","  feature_dim = 0\n","  words_in_embeddings = []\n","  for word in embeddings.wv.vocab:\n","    words_in_embeddings.append(word)\n","  for chosen_word_id in chosen_words:\n","\n","    print(feature_dim)\n","    curr_word = training_dict_words[chosen_word_id]\n","    sim_array = []\n","    for word in words_in_embeddings:\n","      sim_value = cossim(embeddings[word], embeddings[curr_word])\n","      sim_array.append(sim_value)\n","    sorted_sim_array = sorted(sim_array)\n","    threshold1 = sorted_sim_array[int(len(sorted_sim_array)/3)]\n","    threshold2 = sorted_sim_array[int(len(sorted_sim_array)/3*2)]\n","    print(threshold1, threshold2)\n","    for word, value in zip(words_in_embeddings, sim_array):\n","      if value < threshold1:\n","        lsh_dict[word][feature_dim] = -1\n","      elif value < threshold2:\n","        lsh_dict[word][feature_dim] = 0\n","      else:\n","        lsh_dict[word][feature_dim] = 1\n","    feature_dim += 1\n","\n","  return lsh_dict, chosen_words\n","\n","lsh_dict1, chosen_words1= create_lsh_3value(model_embeddings128_fast_with_train, training_word_min_count2, feature_size=128)\n"],"outputs":[],"metadata":{"id":"meB3m128ycYe"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict2_file = open(\"lsh_dict_3val128.pickle\", \"wb\")\n","pickle.dump(lsh_dict1, lsh_dict2_file)\n","lsh_dict2_file.close()"],"outputs":[],"metadata":{"id":"eDkNZqQ9ycYf"}},{"cell_type":"code","execution_count":null,"source":["with open('chosen_words_3val128.npy', 'wb') as f:\n","    np.save(f, chosen_words1)"],"outputs":[],"metadata":{"id":"EpttAESFycYf"}},{"cell_type":"code","execution_count":null,"source":["def create_lsh_no_repeat_3value(embeddings, training_dict_words, already_chosen, feature_size=128):\n","  chosen_words = np.random.choice(list(set(np.arange(len(training_dict_words))) - set(already_chosen)), feature_size)\n","  lsh_dict = {}\n","  for word in embeddings.wv.vocab:\n","    lsh_dict[word] = np.zeros((feature_size,), dtype=np.short)\n","  sim_array = []\n","  feature_dim = 0\n","  words_in_embeddings = []\n","  for word in embeddings.wv.vocab:\n","    words_in_embeddings.append(word)\n","  for chosen_word_id in chosen_words:\n","    print(feature_dim)\n","    curr_word = training_dict_words[chosen_word_id]\n","    sim_array = []\n","    for word in words_in_embeddings:\n","      sim_value = cossim(embeddings[word], embeddings[curr_word])\n","      sim_array.append(sim_value)\n","    sorted_sim_array = sorted(sim_array)\n","    threshold1 = sorted_sim_array[int(len(sorted_sim_array)/3)]\n","    threshold2 = sorted_sim_array[int(len(sorted_sim_array)/3*2)]\n","    print(threshold1, threshold2)\n","    for word, value in zip(words_in_embeddings, sim_array):\n","      if value < threshold1:\n","        lsh_dict[word][feature_dim] = -1\n","      elif value < threshold2:\n","        lsh_dict[word][feature_dim] = 0\n","      else:\n","        lsh_dict[word][feature_dim] = 1\n","    feature_dim += 1\n","\n","  return lsh_dict, chosen_words\n","\n","lsh_dict2, chosen_words2= create_lsh_no_repeat_3value(model_embeddings128_fast_with_train, training_word_min_count2, chosen_words1, feature_size=512-128)\n","\n","lsh_dict1_file = open(\"lsh_dict_3val400.pickle\", \"wb\")\n","pickle.dump(lsh_dict2, lsh_dict1_file)\n","lsh_dict1_file.close()\n","\n","\n","with open('chosen_words_3val400.npy', 'wb') as f:\n","    np.save(f, chosen_words1)"],"outputs":[],"metadata":{"id":"4LKKsXIfycYf"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict2['kobieta']"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1,  1,  1,  1,  1,  0,  0, -1, -1,  0,  0,  0,  0, -1, -1,  0,  0,\n","        0,  1,  0,  0, -1, -1,  1,  1,  0,  0, -1,  0,  0,  1,  0,  0, -1,\n","        0,  1,  0,  0,  1, -1,  0,  1, -1, -1,  0,  0,  0,  0,  1,  1, -1,\n","        0,  1,  1, -1, -1, -1, -1,  1,  0, -1,  1,  0,  1,  0, -1,  1,  1,\n","        0,  0,  1,  0,  1,  0, -1,  0,  1, -1,  1, -1,  1, -1,  1, -1,  1,\n","       -1,  0,  0,  0,  1,  0,  0,  1,  1, -1,  0,  1,  1,  0,  1,  1,  1,\n","        0,  1,  1,  1,  0,  1,  0, -1,  0,  1,  0, -1,  1,  1,  0, -1, -1,\n","        0,  0, -1, -1,  0,  0,  1, -1,  1, -1, -1,  1,  0,  1,  1,  0, -1,\n","        1,  1, -1,  1, -1,  1, -1, -1,  0,  1,  0, -1, -1,  0,  1, -1,  1,\n","        0, -1,  1,  1,  1,  0,  0,  1,  1,  0,  1,  0, -1,  1, -1,  1,  0,\n","        0,  1,  0,  1,  1,  1,  1,  0,  1,  1,  0,  0,  1,  0,  1,  0, -1,\n","        1, -1,  0,  0,  1,  0, -1,  0,  0, -1,  1,  1, -1,  0,  1,  0,  0,\n","        1,  0,  1,  1,  0,  0,  0,  0,  0,  1, -1,  1,  0,  1,  1, -1,  0,\n","        1, -1,  0, -1, -1,  1,  0,  0,  1, -1,  0,  0,  1,  0,  0, -1,  0,\n","        1,  1,  0,  1,  0, -1,  1, -1,  1,  1,  0, -1,  1,  1,  1,  1,  1,\n","       -1,  1,  0, -1,  1, -1, -1,  1, -1,  1,  0,  0,  1,  1,  1,  0,  0,\n","        0,  1,  0,  0,  1, -1,  1,  0,  0,  1,  1,  1,  0,  0,  1, -1,  1,\n","        1, -1, -1,  0,  0,  0,  1,  0,  1,  0,  0,  1,  0,  0,  1,  0, -1,\n","        1, -1,  1, -1,  1,  1,  0,  1, -1,  1,  1,  1,  0,  0, -1,  1,  0,\n","       -1,  0,  1,  0,  1,  1,  0, -1,  1,  0, -1, -1, -1,  1,  1,  1,  1,\n","        1,  1,  1,  1,  1,  1, -1,  0,  0, -1,  1, -1,  0,  1, -1,  1,  0,\n","        1,  0, -1,  0,  0,  1,  1, -1,  1,  1, -1,  0,  1,  1, -1,  0, -1,\n","        0,  0,  0, -1, -1,  1,  1,  1,  1,  1], dtype=int16)"]},"metadata":{},"execution_count":37}],"metadata":{"id":"ZsBHvMyqycYg","outputId":"05b6a0b9-8023-4893-e399-a9d973849a71"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict1['kobieta'] "],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1,  1, -1, -1,  1,  0,  0, -1,  0,  0,  1,  0,  1,  1,  0, -1,  0,\n","        0,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1, -1,  1, -1,  0,  0, -1,\n","        0,  0,  1,  0,  1,  0,  0,  0,  0, -1, -1,  0,  0, -1,  0, -1,  0,\n","       -1,  1,  0, -1,  0,  1,  1,  0,  1,  0,  0, -1,  1, -1,  0, -1, -1,\n","       -1,  1,  0,  1, -1,  0,  1, -1,  0, -1, -1,  0,  0,  0,  1,  0, -1,\n","        0,  0, -1,  0, -1, -1,  1,  1,  1,  0,  0, -1,  0,  1,  0,  1,  0,\n","        1, -1,  1, -1,  0,  1,  1, -1, -1,  1,  0, -1,  0,  0,  1, -1,  1,\n","        0, -1,  1,  0,  1,  1, -1, -1,  1], dtype=int16)"]},"metadata":{},"execution_count":39}],"metadata":{"id":"a-x_PNowycYg","outputId":"6c0137e8-1985-4f84-fed1-2e077ed36638"}},{"cell_type":"code","execution_count":null,"source":["np.concatenate((lsh_dict1['kobieta'], lsh_dict2['kobieta'])).shape"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["(512,)"]},"metadata":{},"execution_count":42}],"metadata":{"id":"nL2aMvQBycYg","outputId":"6912bdba-81f4-48c4-9beb-742c7309b133"}},{"cell_type":"code","execution_count":null,"source":["f = open(\"lsh_dict_3val400.pickle\",'rb')\n","lsh_dict2 = pickle.load(f)\n","f.close()"],"outputs":[],"metadata":{"id":"HduFFadDycYh"}},{"cell_type":"code","execution_count":null,"source":["for word in lsh_dict2:\n","  lsh_dict2[word] = np.concatenate((lsh_dict1[word], lsh_dict2[word]))"],"outputs":[],"metadata":{"id":"IFsF_hhnycYh"}},{"cell_type":"code","execution_count":null,"source":["len(lsh_dict2)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["990644"]},"metadata":{},"execution_count":56}],"metadata":{"id":"enfsFHQ7ycYh","outputId":"ebcfa012-4114-42e0-cba9-de998133cb5b"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict2['kobieta'].shape"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["(512,)"]},"metadata":{},"execution_count":55}],"metadata":{"id":"ARR8pf08ycYi","outputId":"b714ea0a-4a3e-4ae5-e598-283521da7f76"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict1_file = open(\"lsh_dict_3val512.pickle\", \"wb\")\n","pickle.dump(lsh_dict2, lsh_dict1_file)\n","lsh_dict1_file.close()"],"outputs":[],"metadata":{"id":"47yzaDTPycYi"}},{"cell_type":"markdown","source":["## LSH with improved processing"],"metadata":{"id":"WEBWis-hycYP"}},{"cell_type":"code","execution_count":null,"source":["dataset_clean = []\n","with open('dataset_clean5.txt', 'r') as file2:\n","  for i, line in enumerate(file2):\n","    dataset_clean.append(line.split())"],"outputs":[],"metadata":{"id":"FbXWbTvBycYR"}},{"cell_type":"code","execution_count":null,"source":["def read_file(file_name): # read given file and create list of phrases\n","  lines = []\n","  with open(file_name, encoding=\"UTF-8-SIG\") as text:\n","    for line in text:\n","      lines.append(line[:-1])\n","  return lines\n","  \n","text = read_file('training_set_clean_only_text.txt')"],"outputs":[],"metadata":{"id":"lKFr5KF9ycYR"}},{"cell_type":"code","execution_count":null,"source":["emojis_to_words = {'CONFOUNDED FACE': 'zmieszanie',\n","                  'CRYING FACE': 'pacz',\n","                  'DISAPPOINTED BUT RELIEVED FACE': 'rozczarowanie',\n","                  'DISAPPOINTED FACE': 'rozczarowanie',\n","                  'EXPRESSIONLESS FACE': 'bezwyrazowy',\n","                  'FACE SAVOURING DELICIOUS FOOD': 'pysznoci',\n","                  'FACE SCREAMING IN FEAR': 'strach',\n","                  'FACE THROWING A KISS': 'pocaunek',\n","                  'FACE WITH COLD SWEAT': 'stres',\n","                  'FACE WITH MEDICAL MASK': 'maska',\n","                  'FACE WITH OK GESTURE': 'ok',\n","                  'FACE WITH OPEN MOUTH': 'zdziwienie',\n","                  'FACE WITH STUCK-OUT TONGUE': 'jzyk',\n","                  'FACE WITH STUCK-OUT TONGUE AND TIGHTLY-CLOSED EYES': 'jzyk',\n","                  'FACE WITH STUCK-OUT TONGUE AND WINKING EYE': 'jzyk',\n","                  'FACE WITH TEARS OF JOY': 'rado',\n","                  'FEARFUL FACE': 'strach',\n","                  'FLUSHED FACE': 'zaczerwienienie',\n","                  'GRIMACING FACE': 'grymas',\n","                  'GRINNING FACE': 'umiech',\n","                  'GRINNING FACE WITH SMILING EYES': 'umiech',\n","                  'HAPPY PERSON RAISING ONE HAND': 'rado',\n","                  'HEAR-NO-EVIL MONKEY': 'mapka',\n","                  'HUSHED FACE': 'uciszenie',\n","                  'LOUDLY CRYING FACE': 'pacz',\n","                  'NEUTRAL FACE': 'neutralno',\n","                  'PENSIVE FACE': 'zamylenie',\n","                  'PERSEVERING FACE': 'wytrwao',\n","                  'PERSON BOWING DEEPLY': 'ukon',\n","                  'PERSON RAISING BOTH HANDS IN CELEBRATION': 'rado',\n","                  'PERSON WITH FOLDED HANDS': 'ok',\n","                  'POUTING FACE': 'dsanie',\n","                  'SEE-NO-EVIL MONKEY': 'mapka',\n","                  'SLIGHTLY SMILING FACE': 'umiech',\n","                  'SMILING FACE WITH HALO': 'umiech',\n","                  'SMILING FACE WITH HEART-SHAPED EYES': 'mio',\n","                  'SMILING FACE WITH OPEN MOUTH': 'umiech',\n","                  'SMILING FACE WITH OPEN MOUTH AND COLD SWEAT': 'umiech',\n","                  'SMILING FACE WITH OPEN MOUTH AND SMILING EYES': 'umiech',\n","                  'SMILING FACE WITH OPEN MOUTH AND TIGHTLY-CLOSED EYES': 'umiech',\n","                  'SMILING FACE WITH SMILING EYES': 'umiech',\n","                  'SMILING FACE WITH SUNGLASSES': 'umiech',\n","                  'SPEAK-NO-EVIL MONKEY': 'mapka',\n","                  'UNAMUSED FACE': 'nierozbawienie',\n","                  'UPSIDE-DOWN FACE': 'umiech',\n","                  'WEARY FACE': 'zmczenie',\n","                  'WINKING FACE': 'mrugnicie'}"],"outputs":[],"metadata":{"id":"eVBK6DTWycYR"}},{"cell_type":"code","execution_count":null,"source":["not_wanted_words = ['@anonymized_account', 'account', 'anonymized' ]\n","\n","emoji_pattern = re.compile(\"[\"\n","          # u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","          u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n","          u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n","          u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n","          u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n","          u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n","          u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n","          u\"\\U00002702-\\U000027B0\"  # Dingbats\n","          u\"\\U000024C2-\\U0001F251\" \n","                            \"]+\", flags=re.UNICODE)\n","\n","only_faces_emoji_pattern = re.compile(\"[\"\n","          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                            \"]+\", flags=re.UNICODE)\n","\n","\n","def word_process_with_emoji(word, previous_word=''):\n","  if word in not_wanted_words:\n","    return False\n","\n","  if emoji_pattern.match(word):\n","    return False\n","  if only_faces_emoji_pattern.match(word):\n","    word = unicodedata.name(word)\n","    if word in emojis_to_words: \n","      word = emojis_to_words[word]\n","    else: return False\n","    if word == previous_word:\n","      return False\n","    return word\n","  word = word.lower()\n","\n","  analysis = morf.analyse(word)\n","  for interpretation in analysis:\n","    word = interpretation[2][1].split(':')[0]\n","    break\n","  word = word.lower()\n","  if len(word)==0:\n","    return False\n","  return word\n","\n","\n","def clean_dataset_with_emoji(dataset):\n","  dataset_clean = []\n","  re_split = re.compile(r'[\\s{}]+'.format(re.escape(punctuation)))\n","  for line in dataset:\n","    row = []\n","    words = re_split.split(line)\n","    if words[0] == 'RT': \n","      continue\n","    new_words = []\n","    for word in words:\n","      if len(word) == 0: continue\n","      if not emoji_pattern.match(word) and not only_faces_emoji_pattern.match(word):\n","        new_words.append(word)\n","      elif len(word) == 1:\n","        new_words.append(word)\n","      else:\n","        word = emoji.get_emoji_regexp().split(word)\n","        word = [i for i in word if len(i)>0]\n","        new_words += word\n","\n","    previous_word = ''\n","    for word in new_words:\n","      word = word_process_with_emoji(word, previous_word)\n","      if word != False:\n","        previous_word = word\n","      if word != False: \n","        row.append(word)\n","    dataset_clean.append(' '.join(row))\n","  return dataset_clean\n","\n","clean_train = clean_dataset_with_emoji(text)"],"outputs":[],"metadata":{"id":"lvzBwaQmycYS"}},{"cell_type":"code","execution_count":null,"source":["for sen in clean_train:\n","  dataset_clean.append(sen.split())\n","len(dataset_clean)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["5009384"]},"metadata":{},"execution_count":6}],"metadata":{"id":"hLUc2I-mycYU","outputId":"cbe8a1b6-653c-41c1-d44d-30b4f44c7dcc"}},{"cell_type":"code","execution_count":null,"source":["model_embeddings128_fast_with_train = gensim.models.Word2Vec.load(\"word2vec_fast_128_with_train__emoji_rerun.model\")"],"outputs":[],"metadata":{"id":"saS7KBm4ycYU"}},{"cell_type":"code","execution_count":null,"source":["def create_lsh(embeddings, training_dict_words, feature_size=128, threshold=0.61):\n","  chosen_words = np.random.choice(np.arange(len(training_dict_words)), feature_size)\n","  lsh_dict = {}\n","  for word in embeddings.wv.vocab:\n","    lsh_dict[word] = np.zeros((feature_size,), dtype=np.short)\n","\n","  feature_dim = 0\n","  words_in_embeddings = set()\n","  for word in embeddings.wv.vocab:\n","    words_in_embeddings.add(word)\n","  for chosen_word_id in chosen_words:\n","    print(feature_dim)\n","    curr_word = training_dict_words[chosen_word_id]\n","    \n","    for word in words_in_embeddings:\n","      sim_value = cossim(embeddings[word], embeddings[curr_word])\n","\n","      if sim_value >= threshold:\n","        lsh_dict[word][feature_dim] = 1\n","      else:\n","        lsh_dict[word][feature_dim] = 0\n","    feature_dim += 1\n","\n","  return lsh_dict, chosen_words\n"],"outputs":[],"metadata":{"id":"5TGFSCVuycYV"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict1, chosen_words1 = create_lsh(model_embeddings128_fast_with_train, training_word_min_count2, feature_size=512)\n","\n","lsh_dict1_file = open(\"lsh_dict1_emoji_rerun.pickle\", \"wb\")\n","pickle.dump(lsh_dict1, lsh_dict1_file)\n","lsh_dict1_file.close()\n","\n","\n","with open('chosen_words1_emoji_rerun.npy', 'wb') as f:\n","    np.save(f, chosen_words1)"],"outputs":[],"metadata":{"id":"sL5jwF7tycYW"}},{"cell_type":"code","execution_count":null,"source":[""],"outputs":[],"metadata":{"id":"vkn_R0ISycYW"}}],"metadata":{"orig_nbformat":4,"language_info":{"name":"python","version":"3.9.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3.9.7 64-bit"},"interpreter":{"hash":"767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"},"colab":{"name":"Thesis_lsh_embeddings.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}