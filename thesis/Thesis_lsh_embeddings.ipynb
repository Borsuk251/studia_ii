{"cells":[{"cell_type":"code","execution_count":null,"source":["import morfeusz2\n","morf = morfeusz2.Morfeusz()\n","import pandas as pd\n","import numpy as np\n","import re\n","\n","import string\n","from string import ascii_lowercase\n","from sklearn.metrics import f1_score, confusion_matrix\n","\n","import gensim\n","from gensim.models import Word2Vec\n","from gensim.models import FastText\n","import unicodedata\n","!pip install emoji\n","import emoji\n","from string import punctuation\n","import pickle"],"outputs":[{"output_type":"stream","name":"stdout","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: emoji in /home/oskar/.local/lib/python3.9/site-packages (1.6.3)\n"]}],"metadata":{"id":"fAbkgRp_ycYC","outputId":"dc47b28a-dbe7-4037-ed21-145c0cf3d89e"}},{"cell_type":"code","source":["!pip install gdown\n","\n","!gdown --id 1kbYmGkMsQGVBdnDStxx3nz79WNh5JXy7 # training_set_clean_only_text.txt\n","# !gdown --id 1XP30I7gKxhY1jOCOFlcIWDQDnZ3Qk9rZ # training_set_clean_only_tags.txt\n","# !gdown --id 1z0Laz8jCGR-GQcMNL2_qKsGnbL4VRRCY # test_set_clean_only_text.txt\n","# !gdown --id 1o_XRCnKlScCrZRk5suzjwzljT7YonQ2q # test_set_clean_only_tags.txt\n","\n","\n","!gdown --id 1gJL8GYTuWj5BXUUFnCy--AE4xEZlDr1u # corpora\n","\n","!gdown --id 1-IeRYiTy5rujiTg97jrEzhCSpdJkoBL1 # dataset4\n","\n","!gdown --id 1IezKpxY0c8OHv0nq44yrUH8pYtHnsnY- # dataset5\n"],"metadata":{"id":"ee064vxSSkYS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LSH with simpler processing"],"metadata":{"id":"USkTjU7vyr1H"}},{"cell_type":"code","execution_count":null,"source":["print(gensim.__version__)"],"outputs":[{"output_type":"stream","name":"stdout","text":["3.6.0\n"]}],"metadata":{"id":"Rio1KcPXycYH","outputId":"d3432c61-7ff4-454e-ab23-82964bdf9690"}},{"cell_type":"code","execution_count":null,"source":["dataset_clean = []\n","with open('dataset_clean4.txt', 'r') as file2:\n","  for i, line in enumerate(file2):\n","    dataset_clean.append(line.split())"],"outputs":[],"metadata":{"id":"QarPzoxgycYH"}},{"cell_type":"code","execution_count":null,"source":["def read_file(file_name): # read given file and create list of phrases\n","  lines = []\n","  with open(file_name, encoding=\"UTF-8-SIG\") as text:\n","    for line in text:\n","      lines.append(line[:-1])\n","  return lines\n","  \n","text = read_file('training_set_clean_only_text.txt')"],"outputs":[],"metadata":{"id":"e5a4ZG-3ycYI"}},{"cell_type":"code","execution_count":null,"source":["text[:10]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Dla mnie faworytem do tytułu będzie Cracovia. Zobaczymy, czy typ się sprawdzi.',\n"," '@anonymized_account @anonymized_account Brawo ty Daria kibic ma być na dobre i złe',\n"," '@anonymized_account @anonymized_account Super, polski premier składa kwiaty na grobach kolaborantów. Ale doczekaliśmy czasów.',\n"," '@anonymized_account @anonymized_account Musi. Innej drogi nie mamy.',\n"," 'Odrzut natychmiastowy, kwaśna mina, mam problem',\n"," 'Jaki on był fajny xdd pamiętam, że spóźniłam się na jego pierwsze zajęcia i to sporo i za karę kazał mi usiąść w pierwszej ławce XD',\n"," '@anonymized_account No nie ma u nas szczęścia 😉',\n"," '@anonymized_account Dawno kogoś tak wrednego nie widziałam xd',\n"," '@anonymized_account @anonymized_account Zaległości były, ale ważne czy były wezwania do zapłaty z których się klub nie wywiązał.',\n"," '@anonymized_account @anonymized_account @anonymized_account Gdzie jest @anonymized_account . Brudziński jesteś kłamcą i marnym kutasem @anonymized_account']"]},"metadata":{},"execution_count":6}],"metadata":{"id":"4nrVhbqGycYI","outputId":"25f39730-8347-470c-92ed-00b5e8d272b6"}},{"cell_type":"code","execution_count":null,"source":["not_wanted_words = ['@anonymized_account']\n","\n","def word_process(word):\n","  if word in not_wanted_words:\n","    return False\n","\n","  emoji_pattern = re.compile(\"[\"\n","          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                            \"]+\", flags=re.UNICODE)\n","  word = emoji_pattern.sub(r'', word) # no emoji\n","  word = word.lower()\n","  analysis = morf.analyse(word)\n","  for interpretation in analysis:\n","    word = interpretation[2][1].split(':')[0]\n","    break \n","  word = word.lower()\n","  if len(word)==0:\n","    return False\n","  return word\n","\n","def clean_dataset(dataset):\n","  dataset_clean = []\n","  for line in dataset:\n","    row = []\n","    words = line.split()\n","    if words[0] == 'RT': \n","      continue\n","    for word in words:\n","      word = word_process(word)\n","      if word != False: \n","        row.append(word)\n","    dataset_clean.append(' '.join(row))\n","  return dataset_clean\n","\n","clean_train = clean_dataset(text)"],"outputs":[],"metadata":{"id":"pGQpy4iYycYJ"}},{"cell_type":"code","execution_count":null,"source":["clean_train[:10]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["['dla ja faworyt do tytuł być cracovia zobaczyć czy typ się sprawdzić',\n"," 'brawo ty daria kibic mieć być na dobry i złe',\n"," 'super polski premier składać kwiat na grób kolaborant ale doczekać czas',\n"," 'muszy inny drogi nie mama',\n"," 'odrzut natychmiastowy kwaśny mina mama problem',\n"," 'jaki on być fajny xdd pamiętać że spóźnić się na on pierwsze zajęcia i to sporo i za kara kazać księga_micheasza usiąść w pierwszy ławka xd',\n"," 'no nie mieć u my szczęście',\n"," 'dawno kto taka wredny nie widzieć xd',\n"," 'zaległość były ale ważny czy były wezwanie do zapłata z który się klub nie wywiązać',\n"," 'gdzie być . brudziński być kłamca i marny kutas']"]},"metadata":{},"execution_count":5}],"metadata":{"id":"eotntFlUycYK","outputId":"59c6d5c1-f59d-41b0-908a-b5ac061d82b1"}},{"cell_type":"code","execution_count":null,"source":["for sen in clean_train:\n","  dataset_clean.append(sen.split())\n","len(dataset_clean)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["5009385"]},"metadata":{},"execution_count":6}],"metadata":{"id":"VWXgm244ycYL","outputId":"06d629fc-c93d-4a5b-9d88-bb4ec435bdcf"}},{"cell_type":"code","execution_count":null,"source":["def cossim(u,v):\n","  if np.all(u == v):\n","    return 1\n","  norm = np.linalg.norm(u)*np.linalg.norm(v)\n","  cosine = u@v/norm\n","  ang = np.arccos(cosine)\n","  return 1-ang/np.pi"],"outputs":[],"metadata":{"id":"FoSGx2ivycYL"}},{"cell_type":"code","execution_count":null,"source":["model_embeddings128_fast_with_train = gensim.models.Word2Vec.load(\"word2vec_fast_128_with_train.model\")"],"outputs":[],"metadata":{"id":"N5m04l99ycYM"}},{"cell_type":"code","execution_count":null,"source":["cossim(model_embeddings128_fast_with_train['kobieta'], model_embeddings128_fast_with_train['1'])"],"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipykernel_42608/158147224.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  cossim(model_embeddings128_fast_with_train['kobieta'], model_embeddings128_fast_with_train['1'])\n"]},{"output_type":"execute_result","data":{"text/plain":["0.5876831846296927"]},"metadata":{},"execution_count":8}],"metadata":{"id":"y1GO2WFfycYM","outputId":"76e60f95-7919-49f6-892b-4b3fd25e12db"}},{"cell_type":"code","execution_count":null,"source":["def create_trainings_dict(train_data):\n","  training_dict = dict()\n","  for sen in train_data:\n","    sen = sen.split()\n","    for word in sen:\n","      if word in training_dict:\n","        training_dict[word] +=1 \n","      else:\n","        training_dict[word] = 1\n","\n","  training_word_min_count2 = []\n","  for word in training_dict:\n","    if training_dict[word] >= 2:\n","      training_word_min_count2.append(word)\n","  return training_dict, training_word_min_count2\n","\n","training_dict, training_word_min_count2 = create_trainings_dict(clean_train)"],"outputs":[],"metadata":{"id":"5gNuJxu_ycYM"}},{"cell_type":"code","execution_count":null,"source":["len(training_dict)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["13715"]},"metadata":{},"execution_count":10}],"metadata":{"id":"xY1BWVfJycYN","outputId":"62e170b6-8ad0-4a78-9690-88a1f0edfe5a"}},{"cell_type":"code","execution_count":null,"source":["len(training_word_min_count2)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["5934"]},"metadata":{},"execution_count":11}],"metadata":{"id":"Kp7GgXgeycYN","outputId":"0e56a2cb-5978-4c5a-99fa-553be02da972"}},{"cell_type":"markdown","source":["### train 512 features"],"metadata":{"id":"en9zZ3U5zEAJ"}},{"cell_type":"code","execution_count":null,"source":["def create_lsh(embeddings, training_dict_words, feature_size=128, threshold=0.61):\n","  chosen_words = np.random.choice(np.arange(len(training_dict_words)), feature_size)\n","  embeddings_dict_size = len(embeddings.wv.vocab)\n","  lsh_dict = {}\n","  for word in embeddings.wv.vocab:\n","    lsh_dict[word] = np.zeros((feature_size,), dtype=np.short)\n","\n","  feature_dim = 0\n","  words_in_embeddings = set()\n","  for word in embeddings.wv.vocab:\n","    words_in_embeddings.add(word)\n","  for chosen_word_id in chosen_words:\n","\n","    print(feature_dim)\n","    curr_word = training_dict_words[chosen_word_id]\n","    \n","    for word in words_in_embeddings:\n","      sim_value = cossim(embeddings[word], embeddings[curr_word])\n","\n","      if sim_value >= threshold:\n","        lsh_dict[word][feature_dim] = 1\n","      else:\n","        lsh_dict[word][feature_dim] = 0\n","    feature_dim += 1\n","  return lsh_dict, chosen_words\n","\n","lsh_dict1, chosen_words1 = create_lsh(model_embeddings128_fast_with_train, training_word_min_count2, feature_size=512)\n"],"outputs":[],"metadata":{"id":"VQjXKTnXycYW"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict1_file = open(\"lsh_dict1.pickle\", \"wb\")\n","pickle.dump(lsh_dict1, lsh_dict1_file)\n","lsh_dict1_file.close()"],"outputs":[],"metadata":{"id":"GDkd7FcIycYY"}},{"cell_type":"code","execution_count":null,"source":["with open('chosen_words1.npy', 'wb') as f:\n","    np.save(f, chosen_words1)"],"outputs":[],"metadata":{"id":"2-4eZIGkycYY"}},{"cell_type":"code","execution_count":null,"source":["chosen_words1 = np.load('chosen_words1.npy',allow_pickle=True)"],"outputs":[],"metadata":{"id":"L1NBxvTeycYY"}},{"cell_type":"code","execution_count":null,"source":["len(training_word_min_count2)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["5934"]},"metadata":{},"execution_count":11}],"metadata":{"id":"Cspe0i9SycYY","outputId":"2b77f9d0-9bae-4f21-e66e-e62c13105119"}},{"cell_type":"code","execution_count":null,"source":["chosen_words1[:10]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4837, 1057,  150,  593, 5403, 3743, 1590, 2511, 3111,  744])"]},"metadata":{},"execution_count":10}],"metadata":{"id":"TX7o9TjrycYZ","outputId":"078ee978-4ffc-4cd8-c139-4604058e7b2d"}},{"cell_type":"markdown","source":["### add next 512 features"],"metadata":{"id":"u562AD3nycYZ"}},{"cell_type":"code","execution_count":null,"source":["np.random.choice(list(set(np.arange(len(training_word_min_count2))) - set(chosen_words1)), 10)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 192, 5414, 3448, 3211,  716, 1863,  310, 3123, 2053, 5292])"]},"metadata":{},"execution_count":12}],"metadata":{"id":"dIYzuEXjycYZ","outputId":"2ddbf5d7-96f4-4986-bd26-cda231478fb3"}},{"cell_type":"code","execution_count":null,"source":["def create_lsh_no_repeat(embeddings, training_dict_words, already_chosen, feature_size=128, threshold=0.61):\n","  chosen_words = np.random.choice(list(set(np.arange(len(training_dict_words))) - set(already_chosen)), feature_size)\n","  lsh_dict = {}\n","  for word in embeddings.wv.vocab:\n","    lsh_dict[word] = np.zeros((feature_size,), dtype=np.short)\n","  feature_dim = 0\n","  words_in_embeddings = set()\n","  for word in embeddings.wv.vocab:\n","    words_in_embeddings.add(word)\n","  for chosen_word_id in chosen_words:\n","    print(feature_dim)\n","    curr_word = training_dict_words[chosen_word_id]\n","    for word in words_in_embeddings:\n","      sim_value = cossim(embeddings[word], embeddings[curr_word])\n","      if sim_value >= threshold:\n","        lsh_dict[word][feature_dim] = 1\n","      else:\n","        lsh_dict[word][feature_dim] = 0\n","    feature_dim += 1\n","  return lsh_dict, chosen_words\n","\n","lsh_dict3, chosen_words3 = create_lsh_no_repeat(model_embeddings128_fast_with_train, training_word_min_count2, chosen_words1, feature_size=512)\n"],"outputs":[],"metadata":{"id":"htfTzSXrycYZ"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict3_file = open(\"lsh_dict3.pickle\", \"wb\")\n","pickle.dump(lsh_dict3, lsh_dict3_file)\n","lsh_dict3_file.close()"],"outputs":[],"metadata":{"id":"HN8bWf-wycYa"}},{"cell_type":"code","execution_count":null,"source":["with open('chosen_words3.npy', 'wb') as f:\n","    np.save(f, chosen_words3)"],"outputs":[],"metadata":{"id":"bqJR564RycYb"}},{"cell_type":"code","execution_count":null,"source":["np.concatenate((lsh_dict3['kobieta'], lsh_dict3['kobieta'])).shape"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1024,)"]},"metadata":{},"execution_count":22}],"metadata":{"id":"43fB8jNnycYb","outputId":"e3571ffb-9137-44d0-c1af-38199e639047"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict1['kobieta']"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n","       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n","       0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n","       0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n","       1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n","       1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n","       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n","       0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n","       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n","       0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n","       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n","       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n","       1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n","       0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n","       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n","       0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n","       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n","       1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","       1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n","       0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n","       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n","       0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n","       1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n","       0, 0, 1, 0, 1, 0], dtype=int16)"]},"metadata":{},"execution_count":14}],"metadata":{"id":"dvRg9mQbycYb","outputId":"d288ac02-4dd6-4a47-bd82-f1ce489bd186"}},{"cell_type":"markdown","source":["### run with vector difference as seperation"],"metadata":{"id":"m1D42qk_ycYc"}},{"cell_type":"code","execution_count":null,"source":["def create_lsh2(embeddings, training_dict_words, feature_size=128, threshold=0.61): # with difference in mind\n","  chosen_words = []\n","  lsh_dict = {}\n","  for word in embeddings.wv.vocab:\n","    lsh_dict[word] = np.zeros((feature_size,), dtype=np.short)\n","\n","  words_in_embeddings = set()\n","  for word in embeddings.wv.vocab:\n","    words_in_embeddings.add(word)\n","  for feature_dim in range(feature_size):\n","\n","    chosen_words_curr = np.random.choice(np.arange(len(training_word_min_count2)), 2)\n","    print(chosen_words_curr)\n","    curr_word1 = training_dict_words[chosen_words_curr[0]]\n","    curr_word2 = training_dict_words[chosen_words_curr[1]]\n","\n","    while len(curr_word1) < 2 or len(curr_word2) < 2:\n","      chosen_words_curr = np.random.choice(np.arange(len(training_word_min_count2)), 2)\n","      print(chosen_words_curr)\n","      curr_word1 = training_dict_words[chosen_words_curr[0]]\n","      curr_word2 = training_dict_words[chosen_words_curr[1]]\n","    chosen_words.append([curr_word1, curr_word2])\n","    cur_vec_sub = embeddings[curr_word1] - embeddings[curr_word2]\n","        print(feature_dim)\n","    \n","    for word in words_in_embeddings:\n","      sim_value = cossim(cur_vec_sub, embeddings[word])\n","\n","      if sim_value >= threshold:\n","        lsh_dict[word][feature_dim] = 1\n","      else:\n","        lsh_dict[word][feature_dim] = 0\n","  return lsh_dict, chosen_words\n","\n","lsh_dict2, chosen_words2 = create_lsh2(model_embeddings128_fast_with_train, training_word_min_count2, feature_size=512, threshold=0.47)\n","\n"],"outputs":[],"metadata":{"id":"T0FGkm3tycYc"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict2_file = open(\"lsh_dict2.pickle\", \"wb\")\n","pickle.dump(lsh_dict2, lsh_dict2_file)\n","lsh_dict2_file.close()"],"outputs":[],"metadata":{"id":"7tsXWWSDycYc"}},{"cell_type":"code","execution_count":null,"source":["with open('chosen_words2.npy', 'wb') as f:\n","    np.save(f, chosen_words2)"],"outputs":[],"metadata":{"id":"O5dzSbSoycYd"}},{"cell_type":"code","execution_count":null,"source":[""],"outputs":[],"metadata":{"id":"vb0OdLPQycYe"}},{"cell_type":"markdown","source":["## create vector of 3 values -1 0 1"],"metadata":{"id":"1xrO7ye9ycYe"}},{"cell_type":"code","execution_count":null,"source":["def create_lsh_3value(embeddings, training_dict_words, feature_size=128):\n","  chosen_words = np.random.choice(np.arange(len(training_dict_words)), feature_size)\n","  lsh_dict = {}\n","  for word in embeddings.wv.vocab:\n","    lsh_dict[word] = np.zeros((feature_size,), dtype=np.short)\n","\n","  sim_array = []\n","  feature_dim = 0\n","  words_in_embeddings = []\n","  for word in embeddings.wv.vocab:\n","    words_in_embeddings.append(word)\n","  for chosen_word_id in chosen_words:\n","\n","    print(feature_dim)\n","    curr_word = training_dict_words[chosen_word_id]\n","    sim_array = []\n","    for word in words_in_embeddings:\n","      sim_value = cossim(embeddings[word], embeddings[curr_word])\n","      sim_array.append(sim_value)\n","    sorted_sim_array = sorted(sim_array)\n","    threshold1 = sorted_sim_array[int(len(sorted_sim_array)/3)]\n","    threshold2 = sorted_sim_array[int(len(sorted_sim_array)/3*2)]\n","    print(threshold1, threshold2)\n","    for word, value in zip(words_in_embeddings, sim_array):\n","      if value < threshold1:\n","        lsh_dict[word][feature_dim] = -1\n","      elif value < threshold2:\n","        lsh_dict[word][feature_dim] = 0\n","      else:\n","        lsh_dict[word][feature_dim] = 1\n","    feature_dim += 1\n","\n","  return lsh_dict, chosen_words\n","\n","lsh_dict1, chosen_words1= create_lsh_3value(model_embeddings128_fast_with_train, training_word_min_count2, feature_size=128)\n"],"outputs":[],"metadata":{"id":"meB3m128ycYe"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict2_file = open(\"lsh_dict_3val128.pickle\", \"wb\")\n","pickle.dump(lsh_dict1, lsh_dict2_file)\n","lsh_dict2_file.close()"],"outputs":[],"metadata":{"id":"eDkNZqQ9ycYf"}},{"cell_type":"code","execution_count":null,"source":["with open('chosen_words_3val128.npy', 'wb') as f:\n","    np.save(f, chosen_words1)"],"outputs":[],"metadata":{"id":"EpttAESFycYf"}},{"cell_type":"code","execution_count":null,"source":["def create_lsh_no_repeat_3value(embeddings, training_dict_words, already_chosen, feature_size=128):\n","  chosen_words = np.random.choice(list(set(np.arange(len(training_dict_words))) - set(already_chosen)), feature_size)\n","  lsh_dict = {}\n","  for word in embeddings.wv.vocab:\n","    lsh_dict[word] = np.zeros((feature_size,), dtype=np.short)\n","  sim_array = []\n","  feature_dim = 0\n","  words_in_embeddings = []\n","  for word in embeddings.wv.vocab:\n","    words_in_embeddings.append(word)\n","  for chosen_word_id in chosen_words:\n","    print(feature_dim)\n","    curr_word = training_dict_words[chosen_word_id]\n","    sim_array = []\n","    for word in words_in_embeddings:\n","      sim_value = cossim(embeddings[word], embeddings[curr_word])\n","      sim_array.append(sim_value)\n","    sorted_sim_array = sorted(sim_array)\n","    threshold1 = sorted_sim_array[int(len(sorted_sim_array)/3)]\n","    threshold2 = sorted_sim_array[int(len(sorted_sim_array)/3*2)]\n","    print(threshold1, threshold2)\n","    for word, value in zip(words_in_embeddings, sim_array):\n","      if value < threshold1:\n","        lsh_dict[word][feature_dim] = -1\n","      elif value < threshold2:\n","        lsh_dict[word][feature_dim] = 0\n","      else:\n","        lsh_dict[word][feature_dim] = 1\n","    feature_dim += 1\n","\n","  return lsh_dict, chosen_words\n","\n","lsh_dict2, chosen_words2= create_lsh_no_repeat_3value(model_embeddings128_fast_with_train, training_word_min_count2, chosen_words1, feature_size=512-128)\n","\n","lsh_dict1_file = open(\"lsh_dict_3val400.pickle\", \"wb\")\n","pickle.dump(lsh_dict2, lsh_dict1_file)\n","lsh_dict1_file.close()\n","\n","\n","with open('chosen_words_3val400.npy', 'wb') as f:\n","    np.save(f, chosen_words1)"],"outputs":[],"metadata":{"id":"4LKKsXIfycYf"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict2['kobieta']"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1,  1,  1,  1,  1,  0,  0, -1, -1,  0,  0,  0,  0, -1, -1,  0,  0,\n","        0,  1,  0,  0, -1, -1,  1,  1,  0,  0, -1,  0,  0,  1,  0,  0, -1,\n","        0,  1,  0,  0,  1, -1,  0,  1, -1, -1,  0,  0,  0,  0,  1,  1, -1,\n","        0,  1,  1, -1, -1, -1, -1,  1,  0, -1,  1,  0,  1,  0, -1,  1,  1,\n","        0,  0,  1,  0,  1,  0, -1,  0,  1, -1,  1, -1,  1, -1,  1, -1,  1,\n","       -1,  0,  0,  0,  1,  0,  0,  1,  1, -1,  0,  1,  1,  0,  1,  1,  1,\n","        0,  1,  1,  1,  0,  1,  0, -1,  0,  1,  0, -1,  1,  1,  0, -1, -1,\n","        0,  0, -1, -1,  0,  0,  1, -1,  1, -1, -1,  1,  0,  1,  1,  0, -1,\n","        1,  1, -1,  1, -1,  1, -1, -1,  0,  1,  0, -1, -1,  0,  1, -1,  1,\n","        0, -1,  1,  1,  1,  0,  0,  1,  1,  0,  1,  0, -1,  1, -1,  1,  0,\n","        0,  1,  0,  1,  1,  1,  1,  0,  1,  1,  0,  0,  1,  0,  1,  0, -1,\n","        1, -1,  0,  0,  1,  0, -1,  0,  0, -1,  1,  1, -1,  0,  1,  0,  0,\n","        1,  0,  1,  1,  0,  0,  0,  0,  0,  1, -1,  1,  0,  1,  1, -1,  0,\n","        1, -1,  0, -1, -1,  1,  0,  0,  1, -1,  0,  0,  1,  0,  0, -1,  0,\n","        1,  1,  0,  1,  0, -1,  1, -1,  1,  1,  0, -1,  1,  1,  1,  1,  1,\n","       -1,  1,  0, -1,  1, -1, -1,  1, -1,  1,  0,  0,  1,  1,  1,  0,  0,\n","        0,  1,  0,  0,  1, -1,  1,  0,  0,  1,  1,  1,  0,  0,  1, -1,  1,\n","        1, -1, -1,  0,  0,  0,  1,  0,  1,  0,  0,  1,  0,  0,  1,  0, -1,\n","        1, -1,  1, -1,  1,  1,  0,  1, -1,  1,  1,  1,  0,  0, -1,  1,  0,\n","       -1,  0,  1,  0,  1,  1,  0, -1,  1,  0, -1, -1, -1,  1,  1,  1,  1,\n","        1,  1,  1,  1,  1,  1, -1,  0,  0, -1,  1, -1,  0,  1, -1,  1,  0,\n","        1,  0, -1,  0,  0,  1,  1, -1,  1,  1, -1,  0,  1,  1, -1,  0, -1,\n","        0,  0,  0, -1, -1,  1,  1,  1,  1,  1], dtype=int16)"]},"metadata":{},"execution_count":37}],"metadata":{"id":"ZsBHvMyqycYg","outputId":"05b6a0b9-8023-4893-e399-a9d973849a71"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict1['kobieta'] "],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1,  1, -1, -1,  1,  0,  0, -1,  0,  0,  1,  0,  1,  1,  0, -1,  0,\n","        0,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1, -1,  1, -1,  0,  0, -1,\n","        0,  0,  1,  0,  1,  0,  0,  0,  0, -1, -1,  0,  0, -1,  0, -1,  0,\n","       -1,  1,  0, -1,  0,  1,  1,  0,  1,  0,  0, -1,  1, -1,  0, -1, -1,\n","       -1,  1,  0,  1, -1,  0,  1, -1,  0, -1, -1,  0,  0,  0,  1,  0, -1,\n","        0,  0, -1,  0, -1, -1,  1,  1,  1,  0,  0, -1,  0,  1,  0,  1,  0,\n","        1, -1,  1, -1,  0,  1,  1, -1, -1,  1,  0, -1,  0,  0,  1, -1,  1,\n","        0, -1,  1,  0,  1,  1, -1, -1,  1], dtype=int16)"]},"metadata":{},"execution_count":39}],"metadata":{"id":"a-x_PNowycYg","outputId":"6c0137e8-1985-4f84-fed1-2e077ed36638"}},{"cell_type":"code","execution_count":null,"source":["np.concatenate((lsh_dict1['kobieta'], lsh_dict2['kobieta'])).shape"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["(512,)"]},"metadata":{},"execution_count":42}],"metadata":{"id":"nL2aMvQBycYg","outputId":"6912bdba-81f4-48c4-9beb-742c7309b133"}},{"cell_type":"code","execution_count":null,"source":["f = open(\"lsh_dict_3val400.pickle\",'rb')\n","lsh_dict2 = pickle.load(f)\n","f.close()"],"outputs":[],"metadata":{"id":"HduFFadDycYh"}},{"cell_type":"code","execution_count":null,"source":["for word in lsh_dict2:\n","  lsh_dict2[word] = np.concatenate((lsh_dict1[word], lsh_dict2[word]))"],"outputs":[],"metadata":{"id":"IFsF_hhnycYh"}},{"cell_type":"code","execution_count":null,"source":["len(lsh_dict2)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["990644"]},"metadata":{},"execution_count":56}],"metadata":{"id":"enfsFHQ7ycYh","outputId":"ebcfa012-4114-42e0-cba9-de998133cb5b"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict2['kobieta'].shape"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["(512,)"]},"metadata":{},"execution_count":55}],"metadata":{"id":"ARR8pf08ycYi","outputId":"b714ea0a-4a3e-4ae5-e598-283521da7f76"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict1_file = open(\"lsh_dict_3val512.pickle\", \"wb\")\n","pickle.dump(lsh_dict2, lsh_dict1_file)\n","lsh_dict1_file.close()"],"outputs":[],"metadata":{"id":"47yzaDTPycYi"}},{"cell_type":"markdown","source":["## LSH with improved processing"],"metadata":{"id":"WEBWis-hycYP"}},{"cell_type":"code","execution_count":null,"source":["dataset_clean = []\n","with open('dataset_clean5.txt', 'r') as file2:\n","  for i, line in enumerate(file2):\n","    dataset_clean.append(line.split())"],"outputs":[],"metadata":{"id":"FbXWbTvBycYR"}},{"cell_type":"code","execution_count":null,"source":["def read_file(file_name): # read given file and create list of phrases\n","  lines = []\n","  with open(file_name, encoding=\"UTF-8-SIG\") as text:\n","    for line in text:\n","      lines.append(line[:-1])\n","  return lines\n","  \n","text = read_file('training_set_clean_only_text.txt')"],"outputs":[],"metadata":{"id":"lKFr5KF9ycYR"}},{"cell_type":"code","execution_count":null,"source":["emojis_to_words = {'CONFOUNDED FACE': 'zmieszanie',\n","                  'CRYING FACE': 'płacz',\n","                  'DISAPPOINTED BUT RELIEVED FACE': 'rozczarowanie',\n","                  'DISAPPOINTED FACE': 'rozczarowanie',\n","                  'EXPRESSIONLESS FACE': 'bezwyrazowy',\n","                  'FACE SAVOURING DELICIOUS FOOD': 'pyszności',\n","                  'FACE SCREAMING IN FEAR': 'strach',\n","                  'FACE THROWING A KISS': 'pocałunek',\n","                  'FACE WITH COLD SWEAT': 'stres',\n","                  'FACE WITH MEDICAL MASK': 'maska',\n","                  'FACE WITH OK GESTURE': 'ok',\n","                  'FACE WITH OPEN MOUTH': 'zdziwienie',\n","                  'FACE WITH STUCK-OUT TONGUE': 'język',\n","                  'FACE WITH STUCK-OUT TONGUE AND TIGHTLY-CLOSED EYES': 'język',\n","                  'FACE WITH STUCK-OUT TONGUE AND WINKING EYE': 'język',\n","                  'FACE WITH TEARS OF JOY': 'radość',\n","                  'FEARFUL FACE': 'strach',\n","                  'FLUSHED FACE': 'zaczerwienienie',\n","                  'GRIMACING FACE': 'grymas',\n","                  'GRINNING FACE': 'uśmiech',\n","                  'GRINNING FACE WITH SMILING EYES': 'uśmiech',\n","                  'HAPPY PERSON RAISING ONE HAND': 'radość',\n","                  'HEAR-NO-EVIL MONKEY': 'małpka',\n","                  'HUSHED FACE': 'uciszenie',\n","                  'LOUDLY CRYING FACE': 'płacz',\n","                  'NEUTRAL FACE': 'neutralność',\n","                  'PENSIVE FACE': 'zamyślenie',\n","                  'PERSEVERING FACE': 'wytrwałość',\n","                  'PERSON BOWING DEEPLY': 'ukłon',\n","                  'PERSON RAISING BOTH HANDS IN CELEBRATION': 'radość',\n","                  'PERSON WITH FOLDED HANDS': 'ok',\n","                  'POUTING FACE': 'dąsanie',\n","                  'SEE-NO-EVIL MONKEY': 'małpka',\n","                  'SLIGHTLY SMILING FACE': 'uśmiech',\n","                  'SMILING FACE WITH HALO': 'uśmiech',\n","                  'SMILING FACE WITH HEART-SHAPED EYES': 'miłość',\n","                  'SMILING FACE WITH OPEN MOUTH': 'uśmiech',\n","                  'SMILING FACE WITH OPEN MOUTH AND COLD SWEAT': 'uśmiech',\n","                  'SMILING FACE WITH OPEN MOUTH AND SMILING EYES': 'uśmiech',\n","                  'SMILING FACE WITH OPEN MOUTH AND TIGHTLY-CLOSED EYES': 'uśmiech',\n","                  'SMILING FACE WITH SMILING EYES': 'uśmiech',\n","                  'SMILING FACE WITH SUNGLASSES': 'uśmiech',\n","                  'SPEAK-NO-EVIL MONKEY': 'małpka',\n","                  'UNAMUSED FACE': 'nierozbawienie',\n","                  'UPSIDE-DOWN FACE': 'uśmiech',\n","                  'WEARY FACE': 'zmęczenie',\n","                  'WINKING FACE': 'mrugnięcie'}"],"outputs":[],"metadata":{"id":"eVBK6DTWycYR"}},{"cell_type":"code","execution_count":null,"source":["not_wanted_words = ['@anonymized_account', 'account', 'anonymized' ]\n","\n","emoji_pattern = re.compile(\"[\"\n","          # u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","          u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n","          u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n","          u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n","          u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n","          u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n","          u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n","          u\"\\U00002702-\\U000027B0\"  # Dingbats\n","          u\"\\U000024C2-\\U0001F251\" \n","                            \"]+\", flags=re.UNICODE)\n","\n","only_faces_emoji_pattern = re.compile(\"[\"\n","          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                            \"]+\", flags=re.UNICODE)\n","\n","\n","def word_process_with_emoji(word, previous_word=''):\n","  if word in not_wanted_words:\n","    return False\n","\n","  if emoji_pattern.match(word):\n","    return False\n","  if only_faces_emoji_pattern.match(word):\n","    word = unicodedata.name(word)\n","    if word in emojis_to_words: \n","      word = emojis_to_words[word]\n","    else: return False\n","    if word == previous_word:\n","      return False\n","    return word\n","  word = word.lower()\n","\n","  analysis = morf.analyse(word)\n","  for interpretation in analysis:\n","    word = interpretation[2][1].split(':')[0]\n","    break\n","  word = word.lower()\n","  if len(word)==0:\n","    return False\n","  return word\n","\n","\n","def clean_dataset_with_emoji(dataset):\n","  dataset_clean = []\n","  re_split = re.compile(r'[\\s{}]+'.format(re.escape(punctuation)))\n","  for line in dataset:\n","    row = []\n","    words = re_split.split(line)\n","    if words[0] == 'RT': \n","      continue\n","    new_words = []\n","    for word in words:\n","      if len(word) == 0: continue\n","      if not emoji_pattern.match(word) and not only_faces_emoji_pattern.match(word):\n","        new_words.append(word)\n","      elif len(word) == 1:\n","        new_words.append(word)\n","      else:\n","        word = emoji.get_emoji_regexp().split(word)\n","        word = [i for i in word if len(i)>0]\n","        new_words += word\n","\n","    previous_word = ''\n","    for word in new_words:\n","      word = word_process_with_emoji(word, previous_word)\n","      if word != False:\n","        previous_word = word\n","      if word != False: \n","        row.append(word)\n","    dataset_clean.append(' '.join(row))\n","  return dataset_clean\n","\n","clean_train = clean_dataset_with_emoji(text)"],"outputs":[],"metadata":{"id":"lvzBwaQmycYS"}},{"cell_type":"code","execution_count":null,"source":["for sen in clean_train:\n","  dataset_clean.append(sen.split())\n","len(dataset_clean)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["5009384"]},"metadata":{},"execution_count":6}],"metadata":{"id":"hLUc2I-mycYU","outputId":"cbe8a1b6-653c-41c1-d44d-30b4f44c7dcc"}},{"cell_type":"code","execution_count":null,"source":["model_embeddings128_fast_with_train = gensim.models.Word2Vec.load(\"word2vec_fast_128_with_train__emoji_rerun.model\")"],"outputs":[],"metadata":{"id":"saS7KBm4ycYU"}},{"cell_type":"code","execution_count":null,"source":["def create_lsh(embeddings, training_dict_words, feature_size=128, threshold=0.61):\n","  chosen_words = np.random.choice(np.arange(len(training_dict_words)), feature_size)\n","  lsh_dict = {}\n","  for word in embeddings.wv.vocab:\n","    lsh_dict[word] = np.zeros((feature_size,), dtype=np.short)\n","\n","  feature_dim = 0\n","  words_in_embeddings = set()\n","  for word in embeddings.wv.vocab:\n","    words_in_embeddings.add(word)\n","  for chosen_word_id in chosen_words:\n","    print(feature_dim)\n","    curr_word = training_dict_words[chosen_word_id]\n","    \n","    for word in words_in_embeddings:\n","      sim_value = cossim(embeddings[word], embeddings[curr_word])\n","\n","      if sim_value >= threshold:\n","        lsh_dict[word][feature_dim] = 1\n","      else:\n","        lsh_dict[word][feature_dim] = 0\n","    feature_dim += 1\n","\n","  return lsh_dict, chosen_words\n"],"outputs":[],"metadata":{"id":"5TGFSCVuycYV"}},{"cell_type":"code","execution_count":null,"source":["lsh_dict1, chosen_words1 = create_lsh(model_embeddings128_fast_with_train, training_word_min_count2, feature_size=512)\n","\n","lsh_dict1_file = open(\"lsh_dict1_emoji_rerun.pickle\", \"wb\")\n","pickle.dump(lsh_dict1, lsh_dict1_file)\n","lsh_dict1_file.close()\n","\n","\n","with open('chosen_words1_emoji_rerun.npy', 'wb') as f:\n","    np.save(f, chosen_words1)"],"outputs":[],"metadata":{"id":"sL5jwF7tycYW"}},{"cell_type":"code","execution_count":null,"source":[""],"outputs":[],"metadata":{"id":"vkn_R0ISycYW"}}],"metadata":{"orig_nbformat":4,"language_info":{"name":"python","version":"3.9.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3.9.7 64-bit"},"interpreter":{"hash":"767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"},"colab":{"name":"Thesis_lsh_embeddings.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}