{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "miro_LSTM_Strategy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g4tSC5EI_5L"
      },
      "source": [
        "##Starting code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y4Xl3H6JH7O"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k2nrh_oJWQ6",
        "outputId": "fe5ff2b3-0c11-43a7-c7fb-70172bfd13dd"
      },
      "source": [
        "%cd '/content/drive/My Drive/hajsy'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/hajsy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXsorj96JZL5"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import requests\n",
        "from pprint import pprint\n",
        "import subprocess\n",
        "import datetime as dt\n",
        "import json\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import Input, Model\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Activation, Dense, Dropout, LSTM, Activation, Flatten, Conv1D, Conv2D, MaxPooling1D, concatenate\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow import reshape, expand_dims, squeeze\n",
        "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
        "from xgboost import XGBClassifier\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39Yn2X-YT-d7"
      },
      "source": [
        "instruments = [\"ETHUSDT\", \"ADAUSDT\", \"SOLUSDT\", \"DOTUSDT\", \"LUNAUSDT\", \"XLMUSDT\", \"LTCUSDT\", \"DOGEUSDT\", \"BTCUSDT\"]\n",
        "instrument = \"DOGEUSDT\"\n",
        "\n",
        "# kline_df = pd.read_csv(f\"{instrument}_JAN_OCT.csv\", index_col=0)\n",
        "kline_df = pd.read_csv('data_jan_oct.csv', index_col=0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "tyxkzIW0V7cI",
        "outputId": "44ad5145-fa61-4115-c94f-9dd53df638b6"
      },
      "source": [
        "ml_df = pd.DataFrame()\n",
        "period_len = 60\n",
        "\n",
        "ml_df[f\"t_future\"] = kline_df.open.shift(-1)\n",
        "for i in range(period_len):\n",
        "  ml_df[f\"t_{i}\"] = kline_df.open.shift(i)\n",
        "ml_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>t_future</th>\n",
              "      <th>t_0</th>\n",
              "      <th>t_1</th>\n",
              "      <th>t_2</th>\n",
              "      <th>t_3</th>\n",
              "      <th>t_4</th>\n",
              "      <th>t_5</th>\n",
              "      <th>t_6</th>\n",
              "      <th>t_7</th>\n",
              "      <th>t_8</th>\n",
              "      <th>t_9</th>\n",
              "      <th>t_10</th>\n",
              "      <th>t_11</th>\n",
              "      <th>t_12</th>\n",
              "      <th>t_13</th>\n",
              "      <th>t_14</th>\n",
              "      <th>t_15</th>\n",
              "      <th>t_16</th>\n",
              "      <th>t_17</th>\n",
              "      <th>t_18</th>\n",
              "      <th>t_19</th>\n",
              "      <th>t_20</th>\n",
              "      <th>t_21</th>\n",
              "      <th>t_22</th>\n",
              "      <th>t_23</th>\n",
              "      <th>t_24</th>\n",
              "      <th>t_25</th>\n",
              "      <th>t_26</th>\n",
              "      <th>t_27</th>\n",
              "      <th>t_28</th>\n",
              "      <th>t_29</th>\n",
              "      <th>t_30</th>\n",
              "      <th>t_31</th>\n",
              "      <th>t_32</th>\n",
              "      <th>t_33</th>\n",
              "      <th>t_34</th>\n",
              "      <th>t_35</th>\n",
              "      <th>t_36</th>\n",
              "      <th>t_37</th>\n",
              "      <th>t_38</th>\n",
              "      <th>t_39</th>\n",
              "      <th>t_40</th>\n",
              "      <th>t_41</th>\n",
              "      <th>t_42</th>\n",
              "      <th>t_43</th>\n",
              "      <th>t_44</th>\n",
              "      <th>t_45</th>\n",
              "      <th>t_46</th>\n",
              "      <th>t_47</th>\n",
              "      <th>t_48</th>\n",
              "      <th>t_49</th>\n",
              "      <th>t_50</th>\n",
              "      <th>t_51</th>\n",
              "      <th>t_52</th>\n",
              "      <th>t_53</th>\n",
              "      <th>t_54</th>\n",
              "      <th>t_55</th>\n",
              "      <th>t_56</th>\n",
              "      <th>t_57</th>\n",
              "      <th>t_58</th>\n",
              "      <th>t_59</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:00:00</th>\n",
              "      <td>28961.67</td>\n",
              "      <td>28923.63</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:01:00</th>\n",
              "      <td>29009.54</td>\n",
              "      <td>28961.67</td>\n",
              "      <td>28923.63</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:02:00</th>\n",
              "      <td>28989.68</td>\n",
              "      <td>29009.54</td>\n",
              "      <td>28961.67</td>\n",
              "      <td>28923.63</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:03:00</th>\n",
              "      <td>28982.67</td>\n",
              "      <td>28989.68</td>\n",
              "      <td>29009.54</td>\n",
              "      <td>28961.67</td>\n",
              "      <td>28923.63</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:04:00</th>\n",
              "      <td>28975.65</td>\n",
              "      <td>28982.67</td>\n",
              "      <td>28989.68</td>\n",
              "      <td>29009.54</td>\n",
              "      <td>28961.67</td>\n",
              "      <td>28923.63</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:35:00</th>\n",
              "      <td>60990.29</td>\n",
              "      <td>60973.73</td>\n",
              "      <td>60937.89</td>\n",
              "      <td>60948.56</td>\n",
              "      <td>60942.24</td>\n",
              "      <td>60833.87</td>\n",
              "      <td>60697.43</td>\n",
              "      <td>60686.30</td>\n",
              "      <td>60674.15</td>\n",
              "      <td>60649.21</td>\n",
              "      <td>60641.09</td>\n",
              "      <td>60678.41</td>\n",
              "      <td>60698.52</td>\n",
              "      <td>60659.07</td>\n",
              "      <td>60643.93</td>\n",
              "      <td>60667.67</td>\n",
              "      <td>60630.00</td>\n",
              "      <td>60659.23</td>\n",
              "      <td>60684.98</td>\n",
              "      <td>60725.75</td>\n",
              "      <td>60777.89</td>\n",
              "      <td>60675.88</td>\n",
              "      <td>60674.85</td>\n",
              "      <td>60638.76</td>\n",
              "      <td>60653.36</td>\n",
              "      <td>60650.31</td>\n",
              "      <td>60607.16</td>\n",
              "      <td>60574.51</td>\n",
              "      <td>60590.00</td>\n",
              "      <td>60598.61</td>\n",
              "      <td>60598.17</td>\n",
              "      <td>60579.44</td>\n",
              "      <td>60650.98</td>\n",
              "      <td>60667.94</td>\n",
              "      <td>60701.54</td>\n",
              "      <td>60725.38</td>\n",
              "      <td>60643.09</td>\n",
              "      <td>60729.98</td>\n",
              "      <td>60619.61</td>\n",
              "      <td>60577.58</td>\n",
              "      <td>60494.20</td>\n",
              "      <td>60515.44</td>\n",
              "      <td>60509.98</td>\n",
              "      <td>60489.35</td>\n",
              "      <td>60518.98</td>\n",
              "      <td>60508.40</td>\n",
              "      <td>60459.36</td>\n",
              "      <td>60420.77</td>\n",
              "      <td>60457.71</td>\n",
              "      <td>60454.05</td>\n",
              "      <td>60481.84</td>\n",
              "      <td>60423.50</td>\n",
              "      <td>60366.09</td>\n",
              "      <td>60331.36</td>\n",
              "      <td>60314.99</td>\n",
              "      <td>60314.05</td>\n",
              "      <td>60357.59</td>\n",
              "      <td>60345.02</td>\n",
              "      <td>60296.80</td>\n",
              "      <td>60350.10</td>\n",
              "      <td>60304.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:36:00</th>\n",
              "      <td>60988.67</td>\n",
              "      <td>60990.29</td>\n",
              "      <td>60973.73</td>\n",
              "      <td>60937.89</td>\n",
              "      <td>60948.56</td>\n",
              "      <td>60942.24</td>\n",
              "      <td>60833.87</td>\n",
              "      <td>60697.43</td>\n",
              "      <td>60686.30</td>\n",
              "      <td>60674.15</td>\n",
              "      <td>60649.21</td>\n",
              "      <td>60641.09</td>\n",
              "      <td>60678.41</td>\n",
              "      <td>60698.52</td>\n",
              "      <td>60659.07</td>\n",
              "      <td>60643.93</td>\n",
              "      <td>60667.67</td>\n",
              "      <td>60630.00</td>\n",
              "      <td>60659.23</td>\n",
              "      <td>60684.98</td>\n",
              "      <td>60725.75</td>\n",
              "      <td>60777.89</td>\n",
              "      <td>60675.88</td>\n",
              "      <td>60674.85</td>\n",
              "      <td>60638.76</td>\n",
              "      <td>60653.36</td>\n",
              "      <td>60650.31</td>\n",
              "      <td>60607.16</td>\n",
              "      <td>60574.51</td>\n",
              "      <td>60590.00</td>\n",
              "      <td>60598.61</td>\n",
              "      <td>60598.17</td>\n",
              "      <td>60579.44</td>\n",
              "      <td>60650.98</td>\n",
              "      <td>60667.94</td>\n",
              "      <td>60701.54</td>\n",
              "      <td>60725.38</td>\n",
              "      <td>60643.09</td>\n",
              "      <td>60729.98</td>\n",
              "      <td>60619.61</td>\n",
              "      <td>60577.58</td>\n",
              "      <td>60494.20</td>\n",
              "      <td>60515.44</td>\n",
              "      <td>60509.98</td>\n",
              "      <td>60489.35</td>\n",
              "      <td>60518.98</td>\n",
              "      <td>60508.40</td>\n",
              "      <td>60459.36</td>\n",
              "      <td>60420.77</td>\n",
              "      <td>60457.71</td>\n",
              "      <td>60454.05</td>\n",
              "      <td>60481.84</td>\n",
              "      <td>60423.50</td>\n",
              "      <td>60366.09</td>\n",
              "      <td>60331.36</td>\n",
              "      <td>60314.99</td>\n",
              "      <td>60314.05</td>\n",
              "      <td>60357.59</td>\n",
              "      <td>60345.02</td>\n",
              "      <td>60296.80</td>\n",
              "      <td>60350.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:37:00</th>\n",
              "      <td>60939.90</td>\n",
              "      <td>60988.67</td>\n",
              "      <td>60990.29</td>\n",
              "      <td>60973.73</td>\n",
              "      <td>60937.89</td>\n",
              "      <td>60948.56</td>\n",
              "      <td>60942.24</td>\n",
              "      <td>60833.87</td>\n",
              "      <td>60697.43</td>\n",
              "      <td>60686.30</td>\n",
              "      <td>60674.15</td>\n",
              "      <td>60649.21</td>\n",
              "      <td>60641.09</td>\n",
              "      <td>60678.41</td>\n",
              "      <td>60698.52</td>\n",
              "      <td>60659.07</td>\n",
              "      <td>60643.93</td>\n",
              "      <td>60667.67</td>\n",
              "      <td>60630.00</td>\n",
              "      <td>60659.23</td>\n",
              "      <td>60684.98</td>\n",
              "      <td>60725.75</td>\n",
              "      <td>60777.89</td>\n",
              "      <td>60675.88</td>\n",
              "      <td>60674.85</td>\n",
              "      <td>60638.76</td>\n",
              "      <td>60653.36</td>\n",
              "      <td>60650.31</td>\n",
              "      <td>60607.16</td>\n",
              "      <td>60574.51</td>\n",
              "      <td>60590.00</td>\n",
              "      <td>60598.61</td>\n",
              "      <td>60598.17</td>\n",
              "      <td>60579.44</td>\n",
              "      <td>60650.98</td>\n",
              "      <td>60667.94</td>\n",
              "      <td>60701.54</td>\n",
              "      <td>60725.38</td>\n",
              "      <td>60643.09</td>\n",
              "      <td>60729.98</td>\n",
              "      <td>60619.61</td>\n",
              "      <td>60577.58</td>\n",
              "      <td>60494.20</td>\n",
              "      <td>60515.44</td>\n",
              "      <td>60509.98</td>\n",
              "      <td>60489.35</td>\n",
              "      <td>60518.98</td>\n",
              "      <td>60508.40</td>\n",
              "      <td>60459.36</td>\n",
              "      <td>60420.77</td>\n",
              "      <td>60457.71</td>\n",
              "      <td>60454.05</td>\n",
              "      <td>60481.84</td>\n",
              "      <td>60423.50</td>\n",
              "      <td>60366.09</td>\n",
              "      <td>60331.36</td>\n",
              "      <td>60314.99</td>\n",
              "      <td>60314.05</td>\n",
              "      <td>60357.59</td>\n",
              "      <td>60345.02</td>\n",
              "      <td>60296.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:38:00</th>\n",
              "      <td>60940.00</td>\n",
              "      <td>60939.90</td>\n",
              "      <td>60988.67</td>\n",
              "      <td>60990.29</td>\n",
              "      <td>60973.73</td>\n",
              "      <td>60937.89</td>\n",
              "      <td>60948.56</td>\n",
              "      <td>60942.24</td>\n",
              "      <td>60833.87</td>\n",
              "      <td>60697.43</td>\n",
              "      <td>60686.30</td>\n",
              "      <td>60674.15</td>\n",
              "      <td>60649.21</td>\n",
              "      <td>60641.09</td>\n",
              "      <td>60678.41</td>\n",
              "      <td>60698.52</td>\n",
              "      <td>60659.07</td>\n",
              "      <td>60643.93</td>\n",
              "      <td>60667.67</td>\n",
              "      <td>60630.00</td>\n",
              "      <td>60659.23</td>\n",
              "      <td>60684.98</td>\n",
              "      <td>60725.75</td>\n",
              "      <td>60777.89</td>\n",
              "      <td>60675.88</td>\n",
              "      <td>60674.85</td>\n",
              "      <td>60638.76</td>\n",
              "      <td>60653.36</td>\n",
              "      <td>60650.31</td>\n",
              "      <td>60607.16</td>\n",
              "      <td>60574.51</td>\n",
              "      <td>60590.00</td>\n",
              "      <td>60598.61</td>\n",
              "      <td>60598.17</td>\n",
              "      <td>60579.44</td>\n",
              "      <td>60650.98</td>\n",
              "      <td>60667.94</td>\n",
              "      <td>60701.54</td>\n",
              "      <td>60725.38</td>\n",
              "      <td>60643.09</td>\n",
              "      <td>60729.98</td>\n",
              "      <td>60619.61</td>\n",
              "      <td>60577.58</td>\n",
              "      <td>60494.20</td>\n",
              "      <td>60515.44</td>\n",
              "      <td>60509.98</td>\n",
              "      <td>60489.35</td>\n",
              "      <td>60518.98</td>\n",
              "      <td>60508.40</td>\n",
              "      <td>60459.36</td>\n",
              "      <td>60420.77</td>\n",
              "      <td>60457.71</td>\n",
              "      <td>60454.05</td>\n",
              "      <td>60481.84</td>\n",
              "      <td>60423.50</td>\n",
              "      <td>60366.09</td>\n",
              "      <td>60331.36</td>\n",
              "      <td>60314.99</td>\n",
              "      <td>60314.05</td>\n",
              "      <td>60357.59</td>\n",
              "      <td>60345.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:39:00</th>\n",
              "      <td>NaN</td>\n",
              "      <td>60940.00</td>\n",
              "      <td>60939.90</td>\n",
              "      <td>60988.67</td>\n",
              "      <td>60990.29</td>\n",
              "      <td>60973.73</td>\n",
              "      <td>60937.89</td>\n",
              "      <td>60948.56</td>\n",
              "      <td>60942.24</td>\n",
              "      <td>60833.87</td>\n",
              "      <td>60697.43</td>\n",
              "      <td>60686.30</td>\n",
              "      <td>60674.15</td>\n",
              "      <td>60649.21</td>\n",
              "      <td>60641.09</td>\n",
              "      <td>60678.41</td>\n",
              "      <td>60698.52</td>\n",
              "      <td>60659.07</td>\n",
              "      <td>60643.93</td>\n",
              "      <td>60667.67</td>\n",
              "      <td>60630.00</td>\n",
              "      <td>60659.23</td>\n",
              "      <td>60684.98</td>\n",
              "      <td>60725.75</td>\n",
              "      <td>60777.89</td>\n",
              "      <td>60675.88</td>\n",
              "      <td>60674.85</td>\n",
              "      <td>60638.76</td>\n",
              "      <td>60653.36</td>\n",
              "      <td>60650.31</td>\n",
              "      <td>60607.16</td>\n",
              "      <td>60574.51</td>\n",
              "      <td>60590.00</td>\n",
              "      <td>60598.61</td>\n",
              "      <td>60598.17</td>\n",
              "      <td>60579.44</td>\n",
              "      <td>60650.98</td>\n",
              "      <td>60667.94</td>\n",
              "      <td>60701.54</td>\n",
              "      <td>60725.38</td>\n",
              "      <td>60643.09</td>\n",
              "      <td>60729.98</td>\n",
              "      <td>60619.61</td>\n",
              "      <td>60577.58</td>\n",
              "      <td>60494.20</td>\n",
              "      <td>60515.44</td>\n",
              "      <td>60509.98</td>\n",
              "      <td>60489.35</td>\n",
              "      <td>60518.98</td>\n",
              "      <td>60508.40</td>\n",
              "      <td>60459.36</td>\n",
              "      <td>60420.77</td>\n",
              "      <td>60457.71</td>\n",
              "      <td>60454.05</td>\n",
              "      <td>60481.84</td>\n",
              "      <td>60423.50</td>\n",
              "      <td>60366.09</td>\n",
              "      <td>60331.36</td>\n",
              "      <td>60314.99</td>\n",
              "      <td>60314.05</td>\n",
              "      <td>60357.59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>426487 rows Ã— 61 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     t_future       t_0       t_1  ...      t_57      t_58      t_59\n",
              "2021-01-01 00:00:00  28961.67  28923.63       NaN  ...       NaN       NaN       NaN\n",
              "2021-01-01 00:01:00  29009.54  28961.67  28923.63  ...       NaN       NaN       NaN\n",
              "2021-01-01 00:02:00  28989.68  29009.54  28961.67  ...       NaN       NaN       NaN\n",
              "2021-01-01 00:03:00  28982.67  28989.68  29009.54  ...       NaN       NaN       NaN\n",
              "2021-01-01 00:04:00  28975.65  28982.67  28989.68  ...       NaN       NaN       NaN\n",
              "...                       ...       ...       ...  ...       ...       ...       ...\n",
              "2021-10-24 20:35:00  60990.29  60973.73  60937.89  ...  60296.80  60350.10  60304.56\n",
              "2021-10-24 20:36:00  60988.67  60990.29  60973.73  ...  60345.02  60296.80  60350.10\n",
              "2021-10-24 20:37:00  60939.90  60988.67  60990.29  ...  60357.59  60345.02  60296.80\n",
              "2021-10-24 20:38:00  60940.00  60939.90  60988.67  ...  60314.05  60357.59  60345.02\n",
              "2021-10-24 20:39:00       NaN  60940.00  60939.90  ...  60314.99  60314.05  60357.59\n",
              "\n",
              "[426487 rows x 61 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "DInLR_f5Wqzg",
        "outputId": "7ee07a0a-53ba-44d9-aeea-5ad1d888a703"
      },
      "source": [
        "ml_df = ml_df.dropna().astype(float)\n",
        "ml_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>t_future</th>\n",
              "      <th>t_0</th>\n",
              "      <th>t_1</th>\n",
              "      <th>t_2</th>\n",
              "      <th>t_3</th>\n",
              "      <th>t_4</th>\n",
              "      <th>t_5</th>\n",
              "      <th>t_6</th>\n",
              "      <th>t_7</th>\n",
              "      <th>t_8</th>\n",
              "      <th>t_9</th>\n",
              "      <th>t_10</th>\n",
              "      <th>t_11</th>\n",
              "      <th>t_12</th>\n",
              "      <th>t_13</th>\n",
              "      <th>t_14</th>\n",
              "      <th>t_15</th>\n",
              "      <th>t_16</th>\n",
              "      <th>t_17</th>\n",
              "      <th>t_18</th>\n",
              "      <th>t_19</th>\n",
              "      <th>t_20</th>\n",
              "      <th>t_21</th>\n",
              "      <th>t_22</th>\n",
              "      <th>t_23</th>\n",
              "      <th>t_24</th>\n",
              "      <th>t_25</th>\n",
              "      <th>t_26</th>\n",
              "      <th>t_27</th>\n",
              "      <th>t_28</th>\n",
              "      <th>t_29</th>\n",
              "      <th>t_30</th>\n",
              "      <th>t_31</th>\n",
              "      <th>t_32</th>\n",
              "      <th>t_33</th>\n",
              "      <th>t_34</th>\n",
              "      <th>t_35</th>\n",
              "      <th>t_36</th>\n",
              "      <th>t_37</th>\n",
              "      <th>t_38</th>\n",
              "      <th>t_39</th>\n",
              "      <th>t_40</th>\n",
              "      <th>t_41</th>\n",
              "      <th>t_42</th>\n",
              "      <th>t_43</th>\n",
              "      <th>t_44</th>\n",
              "      <th>t_45</th>\n",
              "      <th>t_46</th>\n",
              "      <th>t_47</th>\n",
              "      <th>t_48</th>\n",
              "      <th>t_49</th>\n",
              "      <th>t_50</th>\n",
              "      <th>t_51</th>\n",
              "      <th>t_52</th>\n",
              "      <th>t_53</th>\n",
              "      <th>t_54</th>\n",
              "      <th>t_55</th>\n",
              "      <th>t_56</th>\n",
              "      <th>t_57</th>\n",
              "      <th>t_58</th>\n",
              "      <th>t_59</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:59:00</th>\n",
              "      <td>28995.13</td>\n",
              "      <td>29021.63</td>\n",
              "      <td>29015.94</td>\n",
              "      <td>28967.14</td>\n",
              "      <td>28958.72</td>\n",
              "      <td>28947.10</td>\n",
              "      <td>28966.05</td>\n",
              "      <td>28966.00</td>\n",
              "      <td>28948.92</td>\n",
              "      <td>28951.44</td>\n",
              "      <td>28944.93</td>\n",
              "      <td>28939.60</td>\n",
              "      <td>28938.76</td>\n",
              "      <td>28929.09</td>\n",
              "      <td>28894.50</td>\n",
              "      <td>28930.11</td>\n",
              "      <td>28916.97</td>\n",
              "      <td>28919.99</td>\n",
              "      <td>28913.32</td>\n",
              "      <td>28927.33</td>\n",
              "      <td>28897.97</td>\n",
              "      <td>28901.91</td>\n",
              "      <td>28914.15</td>\n",
              "      <td>28928.70</td>\n",
              "      <td>28931.41</td>\n",
              "      <td>28879.48</td>\n",
              "      <td>28890.00</td>\n",
              "      <td>28872.49</td>\n",
              "      <td>28858.53</td>\n",
              "      <td>28876.31</td>\n",
              "      <td>28836.63</td>\n",
              "      <td>28833.73</td>\n",
              "      <td>28839.58</td>\n",
              "      <td>28844.48</td>\n",
              "      <td>28871.28</td>\n",
              "      <td>28844.88</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28796.56</td>\n",
              "      <td>28796.23</td>\n",
              "      <td>28757.97</td>\n",
              "      <td>28822.17</td>\n",
              "      <td>28823.21</td>\n",
              "      <td>28800.58</td>\n",
              "      <td>28812.34</td>\n",
              "      <td>28769.77</td>\n",
              "      <td>28752.80</td>\n",
              "      <td>28716.85</td>\n",
              "      <td>28838.69</td>\n",
              "      <td>28824.36</td>\n",
              "      <td>28848.69</td>\n",
              "      <td>28858.94</td>\n",
              "      <td>28900.00</td>\n",
              "      <td>28934.84</td>\n",
              "      <td>28943.88</td>\n",
              "      <td>28937.11</td>\n",
              "      <td>28975.65</td>\n",
              "      <td>28982.67</td>\n",
              "      <td>28989.68</td>\n",
              "      <td>29009.54</td>\n",
              "      <td>28961.67</td>\n",
              "      <td>28923.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 01:00:00</th>\n",
              "      <td>28987.62</td>\n",
              "      <td>28995.13</td>\n",
              "      <td>29021.63</td>\n",
              "      <td>29015.94</td>\n",
              "      <td>28967.14</td>\n",
              "      <td>28958.72</td>\n",
              "      <td>28947.10</td>\n",
              "      <td>28966.05</td>\n",
              "      <td>28966.00</td>\n",
              "      <td>28948.92</td>\n",
              "      <td>28951.44</td>\n",
              "      <td>28944.93</td>\n",
              "      <td>28939.60</td>\n",
              "      <td>28938.76</td>\n",
              "      <td>28929.09</td>\n",
              "      <td>28894.50</td>\n",
              "      <td>28930.11</td>\n",
              "      <td>28916.97</td>\n",
              "      <td>28919.99</td>\n",
              "      <td>28913.32</td>\n",
              "      <td>28927.33</td>\n",
              "      <td>28897.97</td>\n",
              "      <td>28901.91</td>\n",
              "      <td>28914.15</td>\n",
              "      <td>28928.70</td>\n",
              "      <td>28931.41</td>\n",
              "      <td>28879.48</td>\n",
              "      <td>28890.00</td>\n",
              "      <td>28872.49</td>\n",
              "      <td>28858.53</td>\n",
              "      <td>28876.31</td>\n",
              "      <td>28836.63</td>\n",
              "      <td>28833.73</td>\n",
              "      <td>28839.58</td>\n",
              "      <td>28844.48</td>\n",
              "      <td>28871.28</td>\n",
              "      <td>28844.88</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28796.56</td>\n",
              "      <td>28796.23</td>\n",
              "      <td>28757.97</td>\n",
              "      <td>28822.17</td>\n",
              "      <td>28823.21</td>\n",
              "      <td>28800.58</td>\n",
              "      <td>28812.34</td>\n",
              "      <td>28769.77</td>\n",
              "      <td>28752.80</td>\n",
              "      <td>28716.85</td>\n",
              "      <td>28838.69</td>\n",
              "      <td>28824.36</td>\n",
              "      <td>28848.69</td>\n",
              "      <td>28858.94</td>\n",
              "      <td>28900.00</td>\n",
              "      <td>28934.84</td>\n",
              "      <td>28943.88</td>\n",
              "      <td>28937.11</td>\n",
              "      <td>28975.65</td>\n",
              "      <td>28982.67</td>\n",
              "      <td>28989.68</td>\n",
              "      <td>29009.54</td>\n",
              "      <td>28961.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 01:01:00</th>\n",
              "      <td>28972.27</td>\n",
              "      <td>28987.62</td>\n",
              "      <td>28995.13</td>\n",
              "      <td>29021.63</td>\n",
              "      <td>29015.94</td>\n",
              "      <td>28967.14</td>\n",
              "      <td>28958.72</td>\n",
              "      <td>28947.10</td>\n",
              "      <td>28966.05</td>\n",
              "      <td>28966.00</td>\n",
              "      <td>28948.92</td>\n",
              "      <td>28951.44</td>\n",
              "      <td>28944.93</td>\n",
              "      <td>28939.60</td>\n",
              "      <td>28938.76</td>\n",
              "      <td>28929.09</td>\n",
              "      <td>28894.50</td>\n",
              "      <td>28930.11</td>\n",
              "      <td>28916.97</td>\n",
              "      <td>28919.99</td>\n",
              "      <td>28913.32</td>\n",
              "      <td>28927.33</td>\n",
              "      <td>28897.97</td>\n",
              "      <td>28901.91</td>\n",
              "      <td>28914.15</td>\n",
              "      <td>28928.70</td>\n",
              "      <td>28931.41</td>\n",
              "      <td>28879.48</td>\n",
              "      <td>28890.00</td>\n",
              "      <td>28872.49</td>\n",
              "      <td>28858.53</td>\n",
              "      <td>28876.31</td>\n",
              "      <td>28836.63</td>\n",
              "      <td>28833.73</td>\n",
              "      <td>28839.58</td>\n",
              "      <td>28844.48</td>\n",
              "      <td>28871.28</td>\n",
              "      <td>28844.88</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28796.56</td>\n",
              "      <td>28796.23</td>\n",
              "      <td>28757.97</td>\n",
              "      <td>28822.17</td>\n",
              "      <td>28823.21</td>\n",
              "      <td>28800.58</td>\n",
              "      <td>28812.34</td>\n",
              "      <td>28769.77</td>\n",
              "      <td>28752.80</td>\n",
              "      <td>28716.85</td>\n",
              "      <td>28838.69</td>\n",
              "      <td>28824.36</td>\n",
              "      <td>28848.69</td>\n",
              "      <td>28858.94</td>\n",
              "      <td>28900.00</td>\n",
              "      <td>28934.84</td>\n",
              "      <td>28943.88</td>\n",
              "      <td>28937.11</td>\n",
              "      <td>28975.65</td>\n",
              "      <td>28982.67</td>\n",
              "      <td>28989.68</td>\n",
              "      <td>29009.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 01:02:00</th>\n",
              "      <td>28998.95</td>\n",
              "      <td>28972.27</td>\n",
              "      <td>28987.62</td>\n",
              "      <td>28995.13</td>\n",
              "      <td>29021.63</td>\n",
              "      <td>29015.94</td>\n",
              "      <td>28967.14</td>\n",
              "      <td>28958.72</td>\n",
              "      <td>28947.10</td>\n",
              "      <td>28966.05</td>\n",
              "      <td>28966.00</td>\n",
              "      <td>28948.92</td>\n",
              "      <td>28951.44</td>\n",
              "      <td>28944.93</td>\n",
              "      <td>28939.60</td>\n",
              "      <td>28938.76</td>\n",
              "      <td>28929.09</td>\n",
              "      <td>28894.50</td>\n",
              "      <td>28930.11</td>\n",
              "      <td>28916.97</td>\n",
              "      <td>28919.99</td>\n",
              "      <td>28913.32</td>\n",
              "      <td>28927.33</td>\n",
              "      <td>28897.97</td>\n",
              "      <td>28901.91</td>\n",
              "      <td>28914.15</td>\n",
              "      <td>28928.70</td>\n",
              "      <td>28931.41</td>\n",
              "      <td>28879.48</td>\n",
              "      <td>28890.00</td>\n",
              "      <td>28872.49</td>\n",
              "      <td>28858.53</td>\n",
              "      <td>28876.31</td>\n",
              "      <td>28836.63</td>\n",
              "      <td>28833.73</td>\n",
              "      <td>28839.58</td>\n",
              "      <td>28844.48</td>\n",
              "      <td>28871.28</td>\n",
              "      <td>28844.88</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28796.56</td>\n",
              "      <td>28796.23</td>\n",
              "      <td>28757.97</td>\n",
              "      <td>28822.17</td>\n",
              "      <td>28823.21</td>\n",
              "      <td>28800.58</td>\n",
              "      <td>28812.34</td>\n",
              "      <td>28769.77</td>\n",
              "      <td>28752.80</td>\n",
              "      <td>28716.85</td>\n",
              "      <td>28838.69</td>\n",
              "      <td>28824.36</td>\n",
              "      <td>28848.69</td>\n",
              "      <td>28858.94</td>\n",
              "      <td>28900.00</td>\n",
              "      <td>28934.84</td>\n",
              "      <td>28943.88</td>\n",
              "      <td>28937.11</td>\n",
              "      <td>28975.65</td>\n",
              "      <td>28982.67</td>\n",
              "      <td>28989.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 01:03:00</th>\n",
              "      <td>28988.16</td>\n",
              "      <td>28998.95</td>\n",
              "      <td>28972.27</td>\n",
              "      <td>28987.62</td>\n",
              "      <td>28995.13</td>\n",
              "      <td>29021.63</td>\n",
              "      <td>29015.94</td>\n",
              "      <td>28967.14</td>\n",
              "      <td>28958.72</td>\n",
              "      <td>28947.10</td>\n",
              "      <td>28966.05</td>\n",
              "      <td>28966.00</td>\n",
              "      <td>28948.92</td>\n",
              "      <td>28951.44</td>\n",
              "      <td>28944.93</td>\n",
              "      <td>28939.60</td>\n",
              "      <td>28938.76</td>\n",
              "      <td>28929.09</td>\n",
              "      <td>28894.50</td>\n",
              "      <td>28930.11</td>\n",
              "      <td>28916.97</td>\n",
              "      <td>28919.99</td>\n",
              "      <td>28913.32</td>\n",
              "      <td>28927.33</td>\n",
              "      <td>28897.97</td>\n",
              "      <td>28901.91</td>\n",
              "      <td>28914.15</td>\n",
              "      <td>28928.70</td>\n",
              "      <td>28931.41</td>\n",
              "      <td>28879.48</td>\n",
              "      <td>28890.00</td>\n",
              "      <td>28872.49</td>\n",
              "      <td>28858.53</td>\n",
              "      <td>28876.31</td>\n",
              "      <td>28836.63</td>\n",
              "      <td>28833.73</td>\n",
              "      <td>28839.58</td>\n",
              "      <td>28844.48</td>\n",
              "      <td>28871.28</td>\n",
              "      <td>28844.88</td>\n",
              "      <td>28835.01</td>\n",
              "      <td>28796.56</td>\n",
              "      <td>28796.23</td>\n",
              "      <td>28757.97</td>\n",
              "      <td>28822.17</td>\n",
              "      <td>28823.21</td>\n",
              "      <td>28800.58</td>\n",
              "      <td>28812.34</td>\n",
              "      <td>28769.77</td>\n",
              "      <td>28752.80</td>\n",
              "      <td>28716.85</td>\n",
              "      <td>28838.69</td>\n",
              "      <td>28824.36</td>\n",
              "      <td>28848.69</td>\n",
              "      <td>28858.94</td>\n",
              "      <td>28900.00</td>\n",
              "      <td>28934.84</td>\n",
              "      <td>28943.88</td>\n",
              "      <td>28937.11</td>\n",
              "      <td>28975.65</td>\n",
              "      <td>28982.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:34:00</th>\n",
              "      <td>60973.73</td>\n",
              "      <td>60937.89</td>\n",
              "      <td>60948.56</td>\n",
              "      <td>60942.24</td>\n",
              "      <td>60833.87</td>\n",
              "      <td>60697.43</td>\n",
              "      <td>60686.30</td>\n",
              "      <td>60674.15</td>\n",
              "      <td>60649.21</td>\n",
              "      <td>60641.09</td>\n",
              "      <td>60678.41</td>\n",
              "      <td>60698.52</td>\n",
              "      <td>60659.07</td>\n",
              "      <td>60643.93</td>\n",
              "      <td>60667.67</td>\n",
              "      <td>60630.00</td>\n",
              "      <td>60659.23</td>\n",
              "      <td>60684.98</td>\n",
              "      <td>60725.75</td>\n",
              "      <td>60777.89</td>\n",
              "      <td>60675.88</td>\n",
              "      <td>60674.85</td>\n",
              "      <td>60638.76</td>\n",
              "      <td>60653.36</td>\n",
              "      <td>60650.31</td>\n",
              "      <td>60607.16</td>\n",
              "      <td>60574.51</td>\n",
              "      <td>60590.00</td>\n",
              "      <td>60598.61</td>\n",
              "      <td>60598.17</td>\n",
              "      <td>60579.44</td>\n",
              "      <td>60650.98</td>\n",
              "      <td>60667.94</td>\n",
              "      <td>60701.54</td>\n",
              "      <td>60725.38</td>\n",
              "      <td>60643.09</td>\n",
              "      <td>60729.98</td>\n",
              "      <td>60619.61</td>\n",
              "      <td>60577.58</td>\n",
              "      <td>60494.20</td>\n",
              "      <td>60515.44</td>\n",
              "      <td>60509.98</td>\n",
              "      <td>60489.35</td>\n",
              "      <td>60518.98</td>\n",
              "      <td>60508.40</td>\n",
              "      <td>60459.36</td>\n",
              "      <td>60420.77</td>\n",
              "      <td>60457.71</td>\n",
              "      <td>60454.05</td>\n",
              "      <td>60481.84</td>\n",
              "      <td>60423.50</td>\n",
              "      <td>60366.09</td>\n",
              "      <td>60331.36</td>\n",
              "      <td>60314.99</td>\n",
              "      <td>60314.05</td>\n",
              "      <td>60357.59</td>\n",
              "      <td>60345.02</td>\n",
              "      <td>60296.80</td>\n",
              "      <td>60350.10</td>\n",
              "      <td>60304.56</td>\n",
              "      <td>60283.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:35:00</th>\n",
              "      <td>60990.29</td>\n",
              "      <td>60973.73</td>\n",
              "      <td>60937.89</td>\n",
              "      <td>60948.56</td>\n",
              "      <td>60942.24</td>\n",
              "      <td>60833.87</td>\n",
              "      <td>60697.43</td>\n",
              "      <td>60686.30</td>\n",
              "      <td>60674.15</td>\n",
              "      <td>60649.21</td>\n",
              "      <td>60641.09</td>\n",
              "      <td>60678.41</td>\n",
              "      <td>60698.52</td>\n",
              "      <td>60659.07</td>\n",
              "      <td>60643.93</td>\n",
              "      <td>60667.67</td>\n",
              "      <td>60630.00</td>\n",
              "      <td>60659.23</td>\n",
              "      <td>60684.98</td>\n",
              "      <td>60725.75</td>\n",
              "      <td>60777.89</td>\n",
              "      <td>60675.88</td>\n",
              "      <td>60674.85</td>\n",
              "      <td>60638.76</td>\n",
              "      <td>60653.36</td>\n",
              "      <td>60650.31</td>\n",
              "      <td>60607.16</td>\n",
              "      <td>60574.51</td>\n",
              "      <td>60590.00</td>\n",
              "      <td>60598.61</td>\n",
              "      <td>60598.17</td>\n",
              "      <td>60579.44</td>\n",
              "      <td>60650.98</td>\n",
              "      <td>60667.94</td>\n",
              "      <td>60701.54</td>\n",
              "      <td>60725.38</td>\n",
              "      <td>60643.09</td>\n",
              "      <td>60729.98</td>\n",
              "      <td>60619.61</td>\n",
              "      <td>60577.58</td>\n",
              "      <td>60494.20</td>\n",
              "      <td>60515.44</td>\n",
              "      <td>60509.98</td>\n",
              "      <td>60489.35</td>\n",
              "      <td>60518.98</td>\n",
              "      <td>60508.40</td>\n",
              "      <td>60459.36</td>\n",
              "      <td>60420.77</td>\n",
              "      <td>60457.71</td>\n",
              "      <td>60454.05</td>\n",
              "      <td>60481.84</td>\n",
              "      <td>60423.50</td>\n",
              "      <td>60366.09</td>\n",
              "      <td>60331.36</td>\n",
              "      <td>60314.99</td>\n",
              "      <td>60314.05</td>\n",
              "      <td>60357.59</td>\n",
              "      <td>60345.02</td>\n",
              "      <td>60296.80</td>\n",
              "      <td>60350.10</td>\n",
              "      <td>60304.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:36:00</th>\n",
              "      <td>60988.67</td>\n",
              "      <td>60990.29</td>\n",
              "      <td>60973.73</td>\n",
              "      <td>60937.89</td>\n",
              "      <td>60948.56</td>\n",
              "      <td>60942.24</td>\n",
              "      <td>60833.87</td>\n",
              "      <td>60697.43</td>\n",
              "      <td>60686.30</td>\n",
              "      <td>60674.15</td>\n",
              "      <td>60649.21</td>\n",
              "      <td>60641.09</td>\n",
              "      <td>60678.41</td>\n",
              "      <td>60698.52</td>\n",
              "      <td>60659.07</td>\n",
              "      <td>60643.93</td>\n",
              "      <td>60667.67</td>\n",
              "      <td>60630.00</td>\n",
              "      <td>60659.23</td>\n",
              "      <td>60684.98</td>\n",
              "      <td>60725.75</td>\n",
              "      <td>60777.89</td>\n",
              "      <td>60675.88</td>\n",
              "      <td>60674.85</td>\n",
              "      <td>60638.76</td>\n",
              "      <td>60653.36</td>\n",
              "      <td>60650.31</td>\n",
              "      <td>60607.16</td>\n",
              "      <td>60574.51</td>\n",
              "      <td>60590.00</td>\n",
              "      <td>60598.61</td>\n",
              "      <td>60598.17</td>\n",
              "      <td>60579.44</td>\n",
              "      <td>60650.98</td>\n",
              "      <td>60667.94</td>\n",
              "      <td>60701.54</td>\n",
              "      <td>60725.38</td>\n",
              "      <td>60643.09</td>\n",
              "      <td>60729.98</td>\n",
              "      <td>60619.61</td>\n",
              "      <td>60577.58</td>\n",
              "      <td>60494.20</td>\n",
              "      <td>60515.44</td>\n",
              "      <td>60509.98</td>\n",
              "      <td>60489.35</td>\n",
              "      <td>60518.98</td>\n",
              "      <td>60508.40</td>\n",
              "      <td>60459.36</td>\n",
              "      <td>60420.77</td>\n",
              "      <td>60457.71</td>\n",
              "      <td>60454.05</td>\n",
              "      <td>60481.84</td>\n",
              "      <td>60423.50</td>\n",
              "      <td>60366.09</td>\n",
              "      <td>60331.36</td>\n",
              "      <td>60314.99</td>\n",
              "      <td>60314.05</td>\n",
              "      <td>60357.59</td>\n",
              "      <td>60345.02</td>\n",
              "      <td>60296.80</td>\n",
              "      <td>60350.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:37:00</th>\n",
              "      <td>60939.90</td>\n",
              "      <td>60988.67</td>\n",
              "      <td>60990.29</td>\n",
              "      <td>60973.73</td>\n",
              "      <td>60937.89</td>\n",
              "      <td>60948.56</td>\n",
              "      <td>60942.24</td>\n",
              "      <td>60833.87</td>\n",
              "      <td>60697.43</td>\n",
              "      <td>60686.30</td>\n",
              "      <td>60674.15</td>\n",
              "      <td>60649.21</td>\n",
              "      <td>60641.09</td>\n",
              "      <td>60678.41</td>\n",
              "      <td>60698.52</td>\n",
              "      <td>60659.07</td>\n",
              "      <td>60643.93</td>\n",
              "      <td>60667.67</td>\n",
              "      <td>60630.00</td>\n",
              "      <td>60659.23</td>\n",
              "      <td>60684.98</td>\n",
              "      <td>60725.75</td>\n",
              "      <td>60777.89</td>\n",
              "      <td>60675.88</td>\n",
              "      <td>60674.85</td>\n",
              "      <td>60638.76</td>\n",
              "      <td>60653.36</td>\n",
              "      <td>60650.31</td>\n",
              "      <td>60607.16</td>\n",
              "      <td>60574.51</td>\n",
              "      <td>60590.00</td>\n",
              "      <td>60598.61</td>\n",
              "      <td>60598.17</td>\n",
              "      <td>60579.44</td>\n",
              "      <td>60650.98</td>\n",
              "      <td>60667.94</td>\n",
              "      <td>60701.54</td>\n",
              "      <td>60725.38</td>\n",
              "      <td>60643.09</td>\n",
              "      <td>60729.98</td>\n",
              "      <td>60619.61</td>\n",
              "      <td>60577.58</td>\n",
              "      <td>60494.20</td>\n",
              "      <td>60515.44</td>\n",
              "      <td>60509.98</td>\n",
              "      <td>60489.35</td>\n",
              "      <td>60518.98</td>\n",
              "      <td>60508.40</td>\n",
              "      <td>60459.36</td>\n",
              "      <td>60420.77</td>\n",
              "      <td>60457.71</td>\n",
              "      <td>60454.05</td>\n",
              "      <td>60481.84</td>\n",
              "      <td>60423.50</td>\n",
              "      <td>60366.09</td>\n",
              "      <td>60331.36</td>\n",
              "      <td>60314.99</td>\n",
              "      <td>60314.05</td>\n",
              "      <td>60357.59</td>\n",
              "      <td>60345.02</td>\n",
              "      <td>60296.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:38:00</th>\n",
              "      <td>60940.00</td>\n",
              "      <td>60939.90</td>\n",
              "      <td>60988.67</td>\n",
              "      <td>60990.29</td>\n",
              "      <td>60973.73</td>\n",
              "      <td>60937.89</td>\n",
              "      <td>60948.56</td>\n",
              "      <td>60942.24</td>\n",
              "      <td>60833.87</td>\n",
              "      <td>60697.43</td>\n",
              "      <td>60686.30</td>\n",
              "      <td>60674.15</td>\n",
              "      <td>60649.21</td>\n",
              "      <td>60641.09</td>\n",
              "      <td>60678.41</td>\n",
              "      <td>60698.52</td>\n",
              "      <td>60659.07</td>\n",
              "      <td>60643.93</td>\n",
              "      <td>60667.67</td>\n",
              "      <td>60630.00</td>\n",
              "      <td>60659.23</td>\n",
              "      <td>60684.98</td>\n",
              "      <td>60725.75</td>\n",
              "      <td>60777.89</td>\n",
              "      <td>60675.88</td>\n",
              "      <td>60674.85</td>\n",
              "      <td>60638.76</td>\n",
              "      <td>60653.36</td>\n",
              "      <td>60650.31</td>\n",
              "      <td>60607.16</td>\n",
              "      <td>60574.51</td>\n",
              "      <td>60590.00</td>\n",
              "      <td>60598.61</td>\n",
              "      <td>60598.17</td>\n",
              "      <td>60579.44</td>\n",
              "      <td>60650.98</td>\n",
              "      <td>60667.94</td>\n",
              "      <td>60701.54</td>\n",
              "      <td>60725.38</td>\n",
              "      <td>60643.09</td>\n",
              "      <td>60729.98</td>\n",
              "      <td>60619.61</td>\n",
              "      <td>60577.58</td>\n",
              "      <td>60494.20</td>\n",
              "      <td>60515.44</td>\n",
              "      <td>60509.98</td>\n",
              "      <td>60489.35</td>\n",
              "      <td>60518.98</td>\n",
              "      <td>60508.40</td>\n",
              "      <td>60459.36</td>\n",
              "      <td>60420.77</td>\n",
              "      <td>60457.71</td>\n",
              "      <td>60454.05</td>\n",
              "      <td>60481.84</td>\n",
              "      <td>60423.50</td>\n",
              "      <td>60366.09</td>\n",
              "      <td>60331.36</td>\n",
              "      <td>60314.99</td>\n",
              "      <td>60314.05</td>\n",
              "      <td>60357.59</td>\n",
              "      <td>60345.02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>426427 rows Ã— 61 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     t_future       t_0       t_1  ...      t_57      t_58      t_59\n",
              "2021-01-01 00:59:00  28995.13  29021.63  29015.94  ...  29009.54  28961.67  28923.63\n",
              "2021-01-01 01:00:00  28987.62  28995.13  29021.63  ...  28989.68  29009.54  28961.67\n",
              "2021-01-01 01:01:00  28972.27  28987.62  28995.13  ...  28982.67  28989.68  29009.54\n",
              "2021-01-01 01:02:00  28998.95  28972.27  28987.62  ...  28975.65  28982.67  28989.68\n",
              "2021-01-01 01:03:00  28988.16  28998.95  28972.27  ...  28937.11  28975.65  28982.67\n",
              "...                       ...       ...       ...  ...       ...       ...       ...\n",
              "2021-10-24 20:34:00  60973.73  60937.89  60948.56  ...  60350.10  60304.56  60283.85\n",
              "2021-10-24 20:35:00  60990.29  60973.73  60937.89  ...  60296.80  60350.10  60304.56\n",
              "2021-10-24 20:36:00  60988.67  60990.29  60973.73  ...  60345.02  60296.80  60350.10\n",
              "2021-10-24 20:37:00  60939.90  60988.67  60990.29  ...  60357.59  60345.02  60296.80\n",
              "2021-10-24 20:38:00  60940.00  60939.90  60988.67  ...  60314.05  60357.59  60345.02\n",
              "\n",
              "[426427 rows x 61 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aubDamwlWr-X",
        "outputId": "9b3016c8-7bc3-45f7-a9c7-e0f3478ac798"
      },
      "source": [
        "time_window = 15 # in minutes, we predict whether the price will raise in next {time_window} minutes\n",
        "transaction_fee = 0.999\n",
        "min_gain = 0.001 # in %\n",
        "\n",
        "last_price = ml_df[f\"t_0\"]# * (1 + min_gain))\n",
        "y = (ml_df[\"t_future\"] * transaction_fee) > last_price\n",
        "\n",
        "# for i in range(time_window):\n",
        "    # y = y | ((ml_df[f\"t_{i}\"] * transaction_fee * transaction_fee) > (ml_df[f\"t_{time_window}\"] * (1 + min_gain)))\n",
        "\n",
        "x = ml_df.drop(columns=[f't_future'])\n",
        "\n",
        "xnp = x.to_numpy()\n",
        "ynp = y.to_numpy()\n",
        "print(xnp.shape)\n",
        "print(ynp.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(426413, 60)\n",
            "(426413,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1QESnFq_dih"
      },
      "source": [
        "## previous\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBUNTrGdcrwy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GO-o_Zpjtzf"
      },
      "source": [
        "def strategy_long_binary_classifier(model, state, price, prev_price, have_stock = False,  money = 1000, stock = 0, time_window = 15, time_with_stock = 0, min_gain = 0.001):\n",
        "    transaction_fee = 0.999\n",
        "    if have_stock:\n",
        "        time_with_stock += 1\n",
        "        if (prev_price * (1+min_gain)) < (price * transaction_fee) or time_with_stock > (time_window*2):\n",
        "            if time_with_stock > time_window:\n",
        "                print(\"SELL STOP LOSS\")\n",
        "            print(f\"SELL : PRICE {price} PREV {prev_price}\")\n",
        "            money += stock * price * transaction_fee\n",
        "            stock = 0\n",
        "            have_stock = False\n",
        "\n",
        "    elif np.any(model.predict(state.reshape(1,-1))):\n",
        "        print(f\"BUY : PRICE {price}\")\n",
        "        time_with_stock = 0\n",
        "        stock = (money / price) * transaction_fee\n",
        "        prev_price = price\n",
        "        money = 0\n",
        "        have_stock = True\n",
        "\n",
        "    return money, stock, have_stock, prev_price, time_with_stock\n",
        "\n",
        "def test_strategy_long(df, model, params = [], period_len = 60, time_window = 15, min_gain = 0.001, train_size = 0.8, test_size = 0.2):\n",
        "    assert(train_size + test_size == 1.0)\n",
        "\n",
        "    transaction_fee = 0.999\n",
        "    ml_df = pd.DataFrame()\n",
        "\n",
        "    for i in range(period_len):\n",
        "        ml_df[f\"t_{i}\"] = df.open.shift(i)\n",
        "    print(ml_df.shape)\n",
        "    ml_df = ml_df.dropna().astype(float)\n",
        "    print(ml_df.shape)\n",
        "\n",
        "    last_prices = (ml_df[f\"t_{time_window}\"] * (1 + min_gain))\n",
        "    y = (ml_df[\"t_0\"] * transaction_fee * transaction_fee) > last_prices\n",
        "\n",
        "    for i in range(time_window):\n",
        "        y = y | ((ml_df[f\"t_{i}\"] * transaction_fee * transaction_fee) > (ml_df[f\"t_{time_window}\"] * (1 + min_gain)))\n",
        "\n",
        "    ml_df = ml_df.drop(columns=[f\"t_{i}\" for i in range(time_window)])\n",
        "\n",
        "    Xnp = ml_df.to_numpy()\n",
        "    ynp = y.to_numpy()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(Xnp, ynp, train_size = train_size , test_size=test_size, shuffle = False)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    money = 1000\n",
        "    stock = 0\n",
        "    num_trans = 0\n",
        "    time_with_stock = 0\n",
        "    have_stock = False\n",
        "    prev_price = 0\n",
        "    sells = []\n",
        "    buys = []\n",
        "    moneys = []\n",
        "\n",
        "    for i in range(int(ml_df.shape[0]*train_size), ml_df.shape[0]):\n",
        "        prev_state = have_stock\n",
        "        money, stock, have_stock, prev_price, time_with_stock = strategy_long_binary_classifier(model, Xnp[i], Xnp[i][0], prev_price, have_stock, money, stock, time_window, time_with_stock, min_gain)\n",
        "\n",
        "        if have_stock != prev_state:\n",
        "            num_trans+=1\n",
        "            if have_stock:\n",
        "                buys.append(i)\n",
        "            else:\n",
        "                sells.append(i)\n",
        "                #print(Xnp[i][0])\n",
        "                #print(ml_df[\"t_15\"][i])\n",
        "                #print(df[period_len-1:].open[i])\n",
        "                moneys.append(money)\n",
        "\n",
        "    return money, stock, num_trans, sells, buys, moneys\n",
        "\n",
        "def buys_sells_plot(df, buys, sells, period_len = 60, time_window = 15):\n",
        "    print(df[period_len-1:].shape)\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=df[period_len-1-time_window:].index, y=df[period_len-1-time_window:].open, mode=\"lines\", name=\"price\"))\n",
        "    fig.add_trace(go.Scatter(x=df[period_len-1-time_window:].index[buys], y=df[period_len-1-time_window:].open[buys], mode=\"markers\", marker=dict(size=12, color=\"green\"), name='Buy signal'))\n",
        "    fig.add_trace(go.Scatter(x=df[].index[sells], y=df[period_len-1-time_window:].open[sells], mode=\"markers\", marker=dict(size=12, color=\"blue\"), name='Sell signal'))\n",
        "    return fig\n",
        "\n",
        "def money_plot(df, sells, moneys, period_len = 60, time_window = 15):\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=df[period_len-1:].index[sells], y=moneys, mode=\"lines\", name=\"price\"))\n",
        "    return fig\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70YA3vKR93ue",
        "outputId": "3d6f2346-c4bc-4ab4-8fc9-d1d86f815fc0"
      },
      "source": [
        "(1007 * (1+min_gain)) < (1002 * transaction_fee * transaction_fee)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHqw3cYz0HvT"
      },
      "source": [
        "instrument = \"DOGEUSDT\"\n",
        "\n",
        "kline_df = pd.read_csv(f\"{instrument}_JAN_OCT.csv\", index_col=0)\n",
        "\n",
        "money, stock, num_trans, sells, buys, moneys = test_strategy_long(kline_df[50000:100000], XGBClassifier(max_depth =5), min_gain=0.003)\n",
        "print(sells)\n",
        "buys_sells_plot(kline_df[50000:100000], buys,sells).show()\n",
        "print(moneys)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxCRb9M3t3-D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9Pt458it5jI"
      },
      "source": [
        "## fixed lstm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI0NpTgE3_4t"
      },
      "source": [
        "\n",
        "period_len=300\n",
        "size_std = 4\n",
        "kline_df[\"ma\"] = kline_df['close'].rolling(period_len).mean() + kline_df['close'].rolling(period_len).std()# 5h hourse\n",
        "kline_df[\"std\"] = kline_df['close'].rolling(period_len).std()\n",
        "kline_df[\"bb_u\"] = kline_df[\"ma\"] + size_std*kline_df[\"std\"]\n",
        "kline_df[\"bb_l\"] = kline_df[\"ma\"] - size_std*kline_df[\"std\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaUgWSVR3_15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237c93a4-426e-41dc-af12-120abd46f5ea"
      },
      "source": [
        "\n",
        "period_len=300\n",
        "size_std = 4\n",
        "kline_df2[\"ma\"] = kline_df2['close'].rolling(period_len).mean() + kline_df2['close'].rolling(period_len).std()# 5h hourse\n",
        "kline_df2[\"std\"] = kline_df2['close'].rolling(period_len).std()\n",
        "kline_df2[\"bb_u\"] = kline_df2[\"ma\"] + size_std*kline_df2[\"std\"]\n",
        "kline_df2[\"bb_l\"] = kline_df2[\"ma\"] - size_std*kline_df2[\"std\"]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zbH6s8Xt3sK"
      },
      "source": [
        "def process_prices(kline_df, time_window=15, period_len=60, target_col='close'):\n",
        "  ml_df = pd.DataFrame()\n",
        "  period_len = period_len\n",
        "  time_window = time_window # in minutes, we predict whether the price will raise in next {time_window} minutes\n",
        "  ml_df[f\"t_future\"] = kline_df[target_col].shift(-time_window)\n",
        "  for i in range(period_len):\n",
        "    ml_df[f\"t_{i}\"] = kline_df[target_col].shift(i)\n",
        "  # ml_df\n",
        "  ml_df = ml_df[500:]\n",
        "  ml_df = ml_df.dropna().astype(float)\n",
        "\n",
        "  transaction_fee = 0.999\n",
        "  min_gain = 0.001 # in %\n",
        "\n",
        "  last_price = ml_df[f\"t_0\"]# * (1 + min_gain))\n",
        "  \n",
        "  y = (ml_df[\"t_future\"] ) > last_price\n",
        "  x = ml_df.drop(columns=[f't_future'])\n",
        "\n",
        "  xnp = x.to_numpy()\n",
        "  ynp = y.to_numpy()\n",
        "  print(xnp.shape)\n",
        "  print(ynp.shape)\n",
        "  # xnp = xnp.reshape(xnp.shape[0], xnp.shape[1], 1)\n",
        "  return xnp, ynp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw0FSVAc0ULf"
      },
      "source": [
        "def add_feature(current_X, new_feature, period_len, omit_num=15, kline_df=kline_df):\n",
        "  ml_df = pd.DataFrame()\n",
        "  period_len = period_len\n",
        "  for i in range(period_len):\n",
        "    ml_df[f\"t_{i}\"] = kline_df[new_feature].shift(i)\n",
        "  ml_df\n",
        "  ml_df = ml_df.dropna().astype(float)\n",
        "  ml_df = ml_df[:-omit_num]\n",
        "  xnp = ml_df.to_numpy()\n",
        "  \n",
        "  print(xnp.shape)\n",
        "  xnp = xnp[-current_X.shape[0]:]\n",
        "  \n",
        "  return np.dstack((current_X, xnp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "O34Eb1Wj5gVQ",
        "outputId": "46c73ec5-bbdf-4ab8-8070-6ef44c06a7b4"
      },
      "source": [
        "kline_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>vol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:00:00</th>\n",
              "      <td>28923.63</td>\n",
              "      <td>28961.66</td>\n",
              "      <td>28913.12</td>\n",
              "      <td>28961.66</td>\n",
              "      <td>27.457032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:01:00</th>\n",
              "      <td>28961.67</td>\n",
              "      <td>29009.91</td>\n",
              "      <td>28961.01</td>\n",
              "      <td>29017.50</td>\n",
              "      <td>58.477501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:02:00</th>\n",
              "      <td>29009.54</td>\n",
              "      <td>28989.30</td>\n",
              "      <td>28973.58</td>\n",
              "      <td>29016.71</td>\n",
              "      <td>42.470329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:03:00</th>\n",
              "      <td>28989.68</td>\n",
              "      <td>28982.69</td>\n",
              "      <td>28972.33</td>\n",
              "      <td>28999.85</td>\n",
              "      <td>30.360677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:04:00</th>\n",
              "      <td>28982.67</td>\n",
              "      <td>28975.65</td>\n",
              "      <td>28971.80</td>\n",
              "      <td>28995.93</td>\n",
              "      <td>24.124339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:35:00</th>\n",
              "      <td>60973.73</td>\n",
              "      <td>60990.30</td>\n",
              "      <td>60942.99</td>\n",
              "      <td>61010.79</td>\n",
              "      <td>26.955470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:36:00</th>\n",
              "      <td>60990.29</td>\n",
              "      <td>60988.67</td>\n",
              "      <td>60967.21</td>\n",
              "      <td>60996.24</td>\n",
              "      <td>15.450520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:37:00</th>\n",
              "      <td>60988.67</td>\n",
              "      <td>60939.90</td>\n",
              "      <td>60939.90</td>\n",
              "      <td>60988.67</td>\n",
              "      <td>39.582950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:38:00</th>\n",
              "      <td>60939.90</td>\n",
              "      <td>60939.99</td>\n",
              "      <td>60928.71</td>\n",
              "      <td>60950.00</td>\n",
              "      <td>11.011360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:39:00</th>\n",
              "      <td>60940.00</td>\n",
              "      <td>60937.03</td>\n",
              "      <td>60935.00</td>\n",
              "      <td>60940.00</td>\n",
              "      <td>3.878300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>426487 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         open      high       low     close        vol\n",
              "2021-01-01 00:00:00  28923.63  28961.66  28913.12  28961.66  27.457032\n",
              "2021-01-01 00:01:00  28961.67  29009.91  28961.01  29017.50  58.477501\n",
              "2021-01-01 00:02:00  29009.54  28989.30  28973.58  29016.71  42.470329\n",
              "2021-01-01 00:03:00  28989.68  28982.69  28972.33  28999.85  30.360677\n",
              "2021-01-01 00:04:00  28982.67  28975.65  28971.80  28995.93  24.124339\n",
              "...                       ...       ...       ...       ...        ...\n",
              "2021-10-24 20:35:00  60973.73  60990.30  60942.99  61010.79  26.955470\n",
              "2021-10-24 20:36:00  60990.29  60988.67  60967.21  60996.24  15.450520\n",
              "2021-10-24 20:37:00  60988.67  60939.90  60939.90  60988.67  39.582950\n",
              "2021-10-24 20:38:00  60939.90  60939.99  60928.71  60950.00  11.011360\n",
              "2021-10-24 20:39:00  60940.00  60937.03  60935.00  60940.00   3.878300\n",
              "\n",
              "[426487 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "cRVc40oGPFAP",
        "outputId": "59580f67-bd15-46c6-9f4c-66a993126129"
      },
      "source": [
        "kline_df2 = kline_df.iloc[::5,:]\n",
        "kline_df2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>vol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:00:00</th>\n",
              "      <td>28923.63</td>\n",
              "      <td>28961.66</td>\n",
              "      <td>28913.12</td>\n",
              "      <td>28961.66</td>\n",
              "      <td>27.457032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:05:00</th>\n",
              "      <td>28975.65</td>\n",
              "      <td>28937.11</td>\n",
              "      <td>28933.16</td>\n",
              "      <td>28979.53</td>\n",
              "      <td>22.396014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:10:00</th>\n",
              "      <td>28858.94</td>\n",
              "      <td>28848.68</td>\n",
              "      <td>28848.06</td>\n",
              "      <td>28883.20</td>\n",
              "      <td>42.665900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:15:00</th>\n",
              "      <td>28752.80</td>\n",
              "      <td>28769.77</td>\n",
              "      <td>28720.91</td>\n",
              "      <td>28775.99</td>\n",
              "      <td>45.695385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:20:00</th>\n",
              "      <td>28822.17</td>\n",
              "      <td>28759.35</td>\n",
              "      <td>28751.58</td>\n",
              "      <td>28822.17</td>\n",
              "      <td>35.799104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:18:00</th>\n",
              "      <td>60684.98</td>\n",
              "      <td>60659.23</td>\n",
              "      <td>60650.00</td>\n",
              "      <td>60700.01</td>\n",
              "      <td>10.283760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:23:00</th>\n",
              "      <td>60659.07</td>\n",
              "      <td>60698.53</td>\n",
              "      <td>60640.90</td>\n",
              "      <td>60700.00</td>\n",
              "      <td>35.601640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:28:00</th>\n",
              "      <td>60674.15</td>\n",
              "      <td>60686.30</td>\n",
              "      <td>60667.06</td>\n",
              "      <td>60692.50</td>\n",
              "      <td>10.890930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:33:00</th>\n",
              "      <td>60948.56</td>\n",
              "      <td>60942.13</td>\n",
              "      <td>60930.05</td>\n",
              "      <td>60998.85</td>\n",
              "      <td>32.688860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:38:00</th>\n",
              "      <td>60939.90</td>\n",
              "      <td>60939.99</td>\n",
              "      <td>60928.71</td>\n",
              "      <td>60950.00</td>\n",
              "      <td>11.011360</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>85298 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         open      high       low     close        vol\n",
              "2021-01-01 00:00:00  28923.63  28961.66  28913.12  28961.66  27.457032\n",
              "2021-01-01 00:05:00  28975.65  28937.11  28933.16  28979.53  22.396014\n",
              "2021-01-01 00:10:00  28858.94  28848.68  28848.06  28883.20  42.665900\n",
              "2021-01-01 00:15:00  28752.80  28769.77  28720.91  28775.99  45.695385\n",
              "2021-01-01 00:20:00  28822.17  28759.35  28751.58  28822.17  35.799104\n",
              "...                       ...       ...       ...       ...        ...\n",
              "2021-10-24 20:18:00  60684.98  60659.23  60650.00  60700.01  10.283760\n",
              "2021-10-24 20:23:00  60659.07  60698.53  60640.90  60700.00  35.601640\n",
              "2021-10-24 20:28:00  60674.15  60686.30  60667.06  60692.50  10.890930\n",
              "2021-10-24 20:33:00  60948.56  60942.13  60930.05  60998.85  32.688860\n",
              "2021-10-24 20:38:00  60939.90  60939.99  60928.71  60950.00  11.011360\n",
              "\n",
              "[85298 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa8MD9MbdLxm",
        "outputId": "856c3830-abab-4fbb-8f9c-3b815e2ca5ec"
      },
      "source": [
        "X, Y = process_prices(kline_df, time_window=1, period_len=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(425986, 30)\n",
            "(425986,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha9vbOTJ6b9H"
      },
      "source": [
        "X = X.reshape((X.shape[0], X.shape[1],1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCi75MEp1OMd",
        "outputId": "6ebcf5c4-4473-48f9-c213-71fa7cb94a05"
      },
      "source": [
        "X3 = add_feature(X, 'high', period_len=30, kline_df=kline_df)\n",
        "X3 = add_feature(X3, 'low', period_len=30, kline_df=kline_df)\n",
        "X3 = add_feature(X3, 'close', period_len=30, kline_df=kline_df)\n",
        "X3 = add_feature(X3, 'vol', period_len=30, kline_df=kline_df)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(426443, 30)\n",
            "(426443, 30)\n",
            "(426443, 30)\n",
            "(426443, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mdfKk5AQSQl"
      },
      "source": [
        "X, Y = process_prices(kline_df2, time_window=3, period_len=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmafFUV61N3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39615b3-f2ff-45b5-bb29-468323e9a4e4"
      },
      "source": [
        "X3.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(425986, 30, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4SCB8xkMrd1"
      },
      "source": [
        "def test_strategy_long_nn(Xnp, ynp, params = [], period_len = 60, time_window = 15, min_gain = 0.001, train_size = 0.8, test_size = 0.2):\n",
        "    assert(train_size + test_size == 1.0)\n",
        "    assert(Xnp.shape[0] == ynp.shape[0])\n",
        "\n",
        "    # transaction_fee = 0.999\n",
        "    # ml_df = pd.DataFrame()\n",
        "\n",
        "    # for i in range(period_len):\n",
        "    #     ml_df[f\"t_{i}\"] = df.open.shift(i)\n",
        "    # print(ml_df.shape)\n",
        "    # ml_df = ml_df.dropna().astype(float)\n",
        "    # print(ml_df.shape)\n",
        "\n",
        "    # last_prices = (ml_df[f\"t_{time_window}\"] * (1 + min_gain))\n",
        "    # y = (ml_df[\"t_0\"] * transaction_fee * transaction_fee) > last_prices\n",
        "\n",
        "    # for i in range(time_window):\n",
        "    #     y = y | ((ml_df[f\"t_{i}\"] * transaction_fee * transaction_fee) > (ml_df[f\"t_{time_window}\"] * (1 + min_gain)))\n",
        "\n",
        "    # ml_df = ml_df.drop(columns=[f\"t_{i}\" for i in range(time_window)])\n",
        "\n",
        "    # Xnp = ml_df.to_numpy()\n",
        "    # ynp = y.to_numpy()\n",
        "    #ynp = to_categorical(ynp)\n",
        "    \n",
        "    ynp = ynp.astype(int)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(Xnp, ynp, train_size = train_size , test_size=test_size, shuffle = False)\n",
        "    \n",
        "    print(X_train.shape)\n",
        "    print(f'fraction is {sum(y_train.astype(int)) / y_train.shape[0]}')\n",
        "\n",
        "    for i in range(X_train.shape[2]):\n",
        "      sc_train = MinMaxScaler(feature_range = (0, 1))\n",
        "      sc_test = MinMaxScaler(feature_range = (0, 1))\n",
        "      X_train[...,i] = sc_train.fit_transform(X_train[...,i])\n",
        "      X_test[...,i] = sc_test.fit_transform(X_test[...,i])\n",
        "      # X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "      # X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "    model = LSTMBasic(X_train)\n",
        "    train_model(model, X_train, X_test, y_train, y_test)\n",
        "    return model, X_train, X_test, y_train, y_test\n",
        "\n",
        "    # money = 1000\n",
        "    # stock = 0\n",
        "    # num_trans = 0\n",
        "    # time_with_stock = 0\n",
        "    # have_stock = False\n",
        "    # prev_price = 0\n",
        "    # sells = []\n",
        "    # buys = []\n",
        "    # moneys = []\n",
        "\n",
        "    # for i in range(int(ml_df.shape[0]*train_size), ml_df.shape[0]):\n",
        "    #     prev_state = have_stock\n",
        "    #     money, stock, have_stock, prev_price, time_with_stock = strategy_long_binary_classifier(model, Xnp[i], Xnp[i][0], prev_price, have_stock, money, stock, time_window, time_with_stock, min_gain)\n",
        "\n",
        "    #     if have_stock != prev_state:\n",
        "    #         num_trans+=1\n",
        "    #         if have_stock:\n",
        "    #             buys.append(i)\n",
        "    #         else:\n",
        "    #             sells.append(i)\n",
        "    #             moneys.append(money)\n",
        "\n",
        "    # return money, stock, num_trans, sells, buys, moneys\n",
        "\n",
        "def train_model(model, X_train, X_test, y_train, y_test):\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "    print(y_train.shape)\n",
        "    print(y_test.shape)\n",
        "\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        model.fit(X_train, y_train, epochs = 300, batch_size = 64)\n",
        "    \n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "    print(sum(y_pred))\n",
        "    # (model.predict(x) > 0.5).astype(\"int32\")\n",
        "    #print(y_pred[:10])\n",
        "    print(y_pred.shape)\n",
        "    print(y_test.shape)\n",
        "    \n",
        "\n",
        "def LSTMBasic(X_train):\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    # model.add(LSTM(units = 100, return_sequences=True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(LSTM(units = 100, return_sequences=True))\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(LSTM(units = 100, return_sequences=True))\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(LSTM(units = 100, return_sequences=False))\n",
        "    # # model.add(Dropout(0.2))\n",
        "    # model.add(Dense(units = 200, activation = \"relu\"))\n",
        "    # model.add(Dense(units = 50, activation = \"relu\"))\n",
        "    # model.add(Dense(units = 1, activation = \"sigmoid\"))\n",
        "\n",
        "    model.add(LSTM(units = 50, return_sequences=True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units = 50, return_sequences=False))\n",
        "    model.add(Dense(units = 1, activation = \"sigmoid\"))\n",
        "    sgd = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "    model.compile(loss = 'binary_crossentropy',  metrics=['accuracy'], optimizer=sgd)\n",
        "    print(model.summary())\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrg234tHwtje",
        "outputId": "f5433162-dba2-4463-d73a-5feb97ac5f38"
      },
      "source": [
        "fraction = 10000\n",
        "print(sum(Y[-fraction:].astype(int)) / Y[-fraction:].shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "82CfHjFUvlOf",
        "outputId": "8d98a0d4-d689-49c4-b5e8-3b58b8cc5475"
      },
      "source": [
        "kline_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>vol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:00:00</th>\n",
              "      <td>28923.63</td>\n",
              "      <td>28961.66</td>\n",
              "      <td>28913.12</td>\n",
              "      <td>28961.66</td>\n",
              "      <td>27.457032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:01:00</th>\n",
              "      <td>28961.67</td>\n",
              "      <td>29009.91</td>\n",
              "      <td>28961.01</td>\n",
              "      <td>29017.50</td>\n",
              "      <td>58.477501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:02:00</th>\n",
              "      <td>29009.54</td>\n",
              "      <td>28989.30</td>\n",
              "      <td>28973.58</td>\n",
              "      <td>29016.71</td>\n",
              "      <td>42.470329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:03:00</th>\n",
              "      <td>28989.68</td>\n",
              "      <td>28982.69</td>\n",
              "      <td>28972.33</td>\n",
              "      <td>28999.85</td>\n",
              "      <td>30.360677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-01-01 00:04:00</th>\n",
              "      <td>28982.67</td>\n",
              "      <td>28975.65</td>\n",
              "      <td>28971.80</td>\n",
              "      <td>28995.93</td>\n",
              "      <td>24.124339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:35:00</th>\n",
              "      <td>60973.73</td>\n",
              "      <td>60990.30</td>\n",
              "      <td>60942.99</td>\n",
              "      <td>61010.79</td>\n",
              "      <td>26.955470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:36:00</th>\n",
              "      <td>60990.29</td>\n",
              "      <td>60988.67</td>\n",
              "      <td>60967.21</td>\n",
              "      <td>60996.24</td>\n",
              "      <td>15.450520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:37:00</th>\n",
              "      <td>60988.67</td>\n",
              "      <td>60939.90</td>\n",
              "      <td>60939.90</td>\n",
              "      <td>60988.67</td>\n",
              "      <td>39.582950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:38:00</th>\n",
              "      <td>60939.90</td>\n",
              "      <td>60939.99</td>\n",
              "      <td>60928.71</td>\n",
              "      <td>60950.00</td>\n",
              "      <td>11.011360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-10-24 20:39:00</th>\n",
              "      <td>60940.00</td>\n",
              "      <td>60937.03</td>\n",
              "      <td>60935.00</td>\n",
              "      <td>60940.00</td>\n",
              "      <td>3.878300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>426487 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         open      high       low     close        vol\n",
              "2021-01-01 00:00:00  28923.63  28961.66  28913.12  28961.66  27.457032\n",
              "2021-01-01 00:01:00  28961.67  29009.91  28961.01  29017.50  58.477501\n",
              "2021-01-01 00:02:00  29009.54  28989.30  28973.58  29016.71  42.470329\n",
              "2021-01-01 00:03:00  28989.68  28982.69  28972.33  28999.85  30.360677\n",
              "2021-01-01 00:04:00  28982.67  28975.65  28971.80  28995.93  24.124339\n",
              "...                       ...       ...       ...       ...        ...\n",
              "2021-10-24 20:35:00  60973.73  60990.30  60942.99  61010.79  26.955470\n",
              "2021-10-24 20:36:00  60990.29  60988.67  60967.21  60996.24  15.450520\n",
              "2021-10-24 20:37:00  60988.67  60939.90  60939.90  60988.67  39.582950\n",
              "2021-10-24 20:38:00  60939.90  60939.99  60928.71  60950.00  11.011360\n",
              "2021-10-24 20:39:00  60940.00  60937.03  60935.00  60940.00   3.878300\n",
              "\n",
              "[426487 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G8nW81Tvhb2",
        "outputId": "12431974-2e29-4336-cb93-bfe718a7f57b"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[66426.12, 66440.3 , 66418.46, ..., 66047.25, 65999.99, 65936.7 ],\n",
              "       [66401.08, 66426.12, 66440.3 , ..., 66026.44, 66047.25, 65999.99],\n",
              "       [66425.12, 66401.08, 66426.12, ..., 65998.46, 66026.44, 66047.25],\n",
              "       ...,\n",
              "       [60990.29, 60973.73, 60937.89, ..., 60131.81, 60133.15, 60194.  ],\n",
              "       [60988.67, 60990.29, 60973.73, ..., 60149.89, 60131.81, 60133.15],\n",
              "       [60939.9 , 60988.67, 60990.29, ..., 60137.36, 60149.89, 60131.81]])"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EnV-t_SwbUd",
        "outputId": "e4e1c6e8-cfd6-46fe-c1eb-3ff2c7804a22"
      },
      "source": [
        "sum(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "553"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvVHUHmHRvL3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPWSCUUW6RKk"
      },
      "source": [
        "## first successful training, simple LSTM network, \n",
        "~40 days training, ~15 testing <br>\n",
        "30 previous minutes <br>\n",
        "5 features - open, close, volume, high, low <br>\n",
        "predicts one minute in the future only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf6FsjsXMC8q",
        "outputId": "9cb06ec8-7771-4fc7-d9d4-ffb34c270582"
      },
      "source": [
        "model, X_train, X_test, y_train, y_test = test_strategy_long_nn(X3[-80000:], Y[-80000:], test_size = 0.3, train_size=0.7)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(56000, 30, 5)\n",
            "fraction is 0.4559285714285714\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 30, 50)            11200     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 50)                20200     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,451\n",
            "Trainable params: 31,451\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "(56000, 30, 5)\n",
            "(24000, 30, 5)\n",
            "(56000,)\n",
            "(24000,)\n",
            "Epoch 1/300\n",
            "875/875 [==============================] - 23s 23ms/step - loss: 0.6898 - accuracy: 0.5432\n",
            "Epoch 2/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6894 - accuracy: 0.5441\n",
            "Epoch 3/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6894 - accuracy: 0.5441\n",
            "Epoch 4/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6894 - accuracy: 0.5441\n",
            "Epoch 5/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 6/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 7/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 8/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 9/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 10/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 11/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 12/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 13/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 14/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 15/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 16/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 17/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6893 - accuracy: 0.5440\n",
            "Epoch 18/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 19/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 20/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 21/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 22/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 23/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 24/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 25/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6893 - accuracy: 0.5441\n",
            "Epoch 26/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6892 - accuracy: 0.5440\n",
            "Epoch 27/300\n",
            "875/875 [==============================] - 23s 26ms/step - loss: 0.6892 - accuracy: 0.5441\n",
            "Epoch 28/300\n",
            "875/875 [==============================] - 23s 26ms/step - loss: 0.6892 - accuracy: 0.5441\n",
            "Epoch 29/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6892 - accuracy: 0.5441\n",
            "Epoch 30/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6892 - accuracy: 0.5441\n",
            "Epoch 31/300\n",
            "875/875 [==============================] - 21s 25ms/step - loss: 0.6892 - accuracy: 0.5442\n",
            "Epoch 32/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6892 - accuracy: 0.5441\n",
            "Epoch 33/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6891 - accuracy: 0.5441\n",
            "Epoch 34/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6891 - accuracy: 0.5442\n",
            "Epoch 35/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6891 - accuracy: 0.5441\n",
            "Epoch 36/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6890 - accuracy: 0.5442\n",
            "Epoch 37/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6891 - accuracy: 0.5442\n",
            "Epoch 38/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6891 - accuracy: 0.5442\n",
            "Epoch 39/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6890 - accuracy: 0.5443\n",
            "Epoch 40/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6890 - accuracy: 0.5442\n",
            "Epoch 41/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6889 - accuracy: 0.5442\n",
            "Epoch 42/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6889 - accuracy: 0.5442\n",
            "Epoch 43/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6890 - accuracy: 0.5442\n",
            "Epoch 44/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6888 - accuracy: 0.5444\n",
            "Epoch 45/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6889 - accuracy: 0.5442\n",
            "Epoch 46/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6888 - accuracy: 0.5444\n",
            "Epoch 47/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6887 - accuracy: 0.5443\n",
            "Epoch 48/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6888 - accuracy: 0.5443\n",
            "Epoch 49/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6886 - accuracy: 0.5445\n",
            "Epoch 50/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6886 - accuracy: 0.5443\n",
            "Epoch 51/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6886 - accuracy: 0.5444\n",
            "Epoch 52/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6886 - accuracy: 0.5445\n",
            "Epoch 53/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6886 - accuracy: 0.5445\n",
            "Epoch 54/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6885 - accuracy: 0.5444\n",
            "Epoch 55/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6887 - accuracy: 0.5445\n",
            "Epoch 56/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6887 - accuracy: 0.5446\n",
            "Epoch 57/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6885 - accuracy: 0.5446\n",
            "Epoch 58/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6885 - accuracy: 0.5446\n",
            "Epoch 59/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6884 - accuracy: 0.5446\n",
            "Epoch 60/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6884 - accuracy: 0.5447\n",
            "Epoch 61/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6883 - accuracy: 0.5444\n",
            "Epoch 62/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6884 - accuracy: 0.5446\n",
            "Epoch 63/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6882 - accuracy: 0.5447\n",
            "Epoch 64/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6882 - accuracy: 0.5448\n",
            "Epoch 65/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6882 - accuracy: 0.5449\n",
            "Epoch 66/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6880 - accuracy: 0.5449\n",
            "Epoch 67/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6881 - accuracy: 0.5451\n",
            "Epoch 68/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6884 - accuracy: 0.5445\n",
            "Epoch 69/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6879 - accuracy: 0.5448\n",
            "Epoch 70/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6877 - accuracy: 0.5451\n",
            "Epoch 71/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6878 - accuracy: 0.5454\n",
            "Epoch 72/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6878 - accuracy: 0.5454\n",
            "Epoch 73/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6876 - accuracy: 0.5454\n",
            "Epoch 74/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6876 - accuracy: 0.5453\n",
            "Epoch 75/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6875 - accuracy: 0.5454\n",
            "Epoch 76/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6873 - accuracy: 0.5452\n",
            "Epoch 77/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6873 - accuracy: 0.5460\n",
            "Epoch 78/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6872 - accuracy: 0.5459\n",
            "Epoch 79/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6872 - accuracy: 0.5462\n",
            "Epoch 80/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6869 - accuracy: 0.5459\n",
            "Epoch 81/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6870 - accuracy: 0.5459\n",
            "Epoch 82/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6868 - accuracy: 0.5461\n",
            "Epoch 83/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6866 - accuracy: 0.5462\n",
            "Epoch 84/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6864 - accuracy: 0.5467\n",
            "Epoch 85/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6863 - accuracy: 0.5464\n",
            "Epoch 86/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6864 - accuracy: 0.5466\n",
            "Epoch 87/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6861 - accuracy: 0.5469\n",
            "Epoch 88/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6861 - accuracy: 0.5473\n",
            "Epoch 89/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6860 - accuracy: 0.5476\n",
            "Epoch 90/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6857 - accuracy: 0.5476\n",
            "Epoch 91/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6854 - accuracy: 0.5476\n",
            "Epoch 92/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6854 - accuracy: 0.5479\n",
            "Epoch 93/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6852 - accuracy: 0.5481\n",
            "Epoch 94/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6850 - accuracy: 0.5480\n",
            "Epoch 95/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6849 - accuracy: 0.5486\n",
            "Epoch 96/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6847 - accuracy: 0.5484\n",
            "Epoch 97/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6857 - accuracy: 0.5482\n",
            "Epoch 98/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6850 - accuracy: 0.5485\n",
            "Epoch 99/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6845 - accuracy: 0.5484\n",
            "Epoch 100/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6843 - accuracy: 0.5494\n",
            "Epoch 101/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6842 - accuracy: 0.5494\n",
            "Epoch 102/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6842 - accuracy: 0.5486\n",
            "Epoch 103/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6836 - accuracy: 0.5492\n",
            "Epoch 104/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6835 - accuracy: 0.5499\n",
            "Epoch 105/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6837 - accuracy: 0.5495\n",
            "Epoch 106/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6833 - accuracy: 0.5491\n",
            "Epoch 107/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6831 - accuracy: 0.5503\n",
            "Epoch 108/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6840 - accuracy: 0.5496\n",
            "Epoch 109/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6829 - accuracy: 0.5504\n",
            "Epoch 110/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6830 - accuracy: 0.5503\n",
            "Epoch 111/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6832 - accuracy: 0.5507\n",
            "Epoch 112/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6825 - accuracy: 0.5511\n",
            "Epoch 113/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6828 - accuracy: 0.5506\n",
            "Epoch 114/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6816 - accuracy: 0.5507\n",
            "Epoch 115/300\n",
            "875/875 [==============================] - 21s 25ms/step - loss: 0.6831 - accuracy: 0.5506\n",
            "Epoch 116/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6840 - accuracy: 0.5501\n",
            "Epoch 117/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6821 - accuracy: 0.5511\n",
            "Epoch 118/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6820 - accuracy: 0.5515\n",
            "Epoch 119/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6825 - accuracy: 0.5508\n",
            "Epoch 120/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6816 - accuracy: 0.5513\n",
            "Epoch 121/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6815 - accuracy: 0.5520\n",
            "Epoch 122/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6811 - accuracy: 0.5521\n",
            "Epoch 123/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6810 - accuracy: 0.5521\n",
            "Epoch 124/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6805 - accuracy: 0.5525\n",
            "Epoch 125/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6809 - accuracy: 0.5521\n",
            "Epoch 126/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6798 - accuracy: 0.5536\n",
            "Epoch 127/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6799 - accuracy: 0.5542\n",
            "Epoch 128/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6810 - accuracy: 0.5530\n",
            "Epoch 129/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6806 - accuracy: 0.5522\n",
            "Epoch 130/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6800 - accuracy: 0.5530\n",
            "Epoch 131/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6799 - accuracy: 0.5529\n",
            "Epoch 132/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6809 - accuracy: 0.5522\n",
            "Epoch 133/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6804 - accuracy: 0.5533\n",
            "Epoch 134/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6797 - accuracy: 0.5537\n",
            "Epoch 135/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6790 - accuracy: 0.5537\n",
            "Epoch 136/300\n",
            "875/875 [==============================] - 21s 25ms/step - loss: 0.6793 - accuracy: 0.5533\n",
            "Epoch 137/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6784 - accuracy: 0.5547\n",
            "Epoch 138/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6786 - accuracy: 0.5539\n",
            "Epoch 139/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6784 - accuracy: 0.5546\n",
            "Epoch 140/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6807 - accuracy: 0.5526\n",
            "Epoch 141/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6783 - accuracy: 0.5540\n",
            "Epoch 142/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6778 - accuracy: 0.5543\n",
            "Epoch 143/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6787 - accuracy: 0.5540\n",
            "Epoch 144/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6770 - accuracy: 0.5548\n",
            "Epoch 145/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6777 - accuracy: 0.5546\n",
            "Epoch 146/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6782 - accuracy: 0.5548\n",
            "Epoch 147/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6774 - accuracy: 0.5551\n",
            "Epoch 148/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6773 - accuracy: 0.5548\n",
            "Epoch 149/300\n",
            "875/875 [==============================] - 21s 25ms/step - loss: 0.6764 - accuracy: 0.5556\n",
            "Epoch 150/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6777 - accuracy: 0.5556\n",
            "Epoch 151/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6768 - accuracy: 0.5551\n",
            "Epoch 152/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6768 - accuracy: 0.5559\n",
            "Epoch 153/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6770 - accuracy: 0.5560\n",
            "Epoch 154/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6766 - accuracy: 0.5564\n",
            "Epoch 155/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6753 - accuracy: 0.5565\n",
            "Epoch 156/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6750 - accuracy: 0.5566\n",
            "Epoch 157/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6752 - accuracy: 0.5568\n",
            "Epoch 158/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6765 - accuracy: 0.5555\n",
            "Epoch 159/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6746 - accuracy: 0.5572\n",
            "Epoch 160/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6755 - accuracy: 0.5563\n",
            "Epoch 161/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6755 - accuracy: 0.5559\n",
            "Epoch 162/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6748 - accuracy: 0.5564\n",
            "Epoch 163/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6742 - accuracy: 0.5579\n",
            "Epoch 164/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6742 - accuracy: 0.5583\n",
            "Epoch 165/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6749 - accuracy: 0.5568\n",
            "Epoch 166/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6750 - accuracy: 0.5580\n",
            "Epoch 167/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6735 - accuracy: 0.5578\n",
            "Epoch 168/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6737 - accuracy: 0.5581\n",
            "Epoch 169/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6755 - accuracy: 0.5574\n",
            "Epoch 170/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6746 - accuracy: 0.5571\n",
            "Epoch 171/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6737 - accuracy: 0.5584\n",
            "Epoch 172/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6725 - accuracy: 0.5586\n",
            "Epoch 173/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6730 - accuracy: 0.5583\n",
            "Epoch 174/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6732 - accuracy: 0.5581\n",
            "Epoch 175/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6728 - accuracy: 0.5593\n",
            "Epoch 176/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6737 - accuracy: 0.5584\n",
            "Epoch 177/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6723 - accuracy: 0.5588\n",
            "Epoch 178/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6732 - accuracy: 0.5585\n",
            "Epoch 179/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6728 - accuracy: 0.5592\n",
            "Epoch 180/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6719 - accuracy: 0.5593\n",
            "Epoch 181/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6712 - accuracy: 0.5592\n",
            "Epoch 182/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6714 - accuracy: 0.5594\n",
            "Epoch 183/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6705 - accuracy: 0.5602\n",
            "Epoch 184/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6723 - accuracy: 0.5600\n",
            "Epoch 185/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6700 - accuracy: 0.5609\n",
            "Epoch 186/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6706 - accuracy: 0.5612\n",
            "Epoch 187/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6732 - accuracy: 0.5589\n",
            "Epoch 188/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6700 - accuracy: 0.5611\n",
            "Epoch 189/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6704 - accuracy: 0.5604\n",
            "Epoch 190/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6732 - accuracy: 0.5588\n",
            "Epoch 191/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6717 - accuracy: 0.5606\n",
            "Epoch 192/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6702 - accuracy: 0.5612\n",
            "Epoch 193/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6698 - accuracy: 0.5617\n",
            "Epoch 194/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6696 - accuracy: 0.5622\n",
            "Epoch 195/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6683 - accuracy: 0.5643\n",
            "Epoch 196/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6707 - accuracy: 0.5619\n",
            "Epoch 197/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6703 - accuracy: 0.5612\n",
            "Epoch 198/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6691 - accuracy: 0.5626\n",
            "Epoch 199/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6705 - accuracy: 0.5617\n",
            "Epoch 200/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6696 - accuracy: 0.5623\n",
            "Epoch 201/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6692 - accuracy: 0.5630\n",
            "Epoch 202/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6685 - accuracy: 0.5633\n",
            "Epoch 203/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6695 - accuracy: 0.5624\n",
            "Epoch 204/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6686 - accuracy: 0.5627\n",
            "Epoch 205/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6686 - accuracy: 0.5623\n",
            "Epoch 206/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6682 - accuracy: 0.5629\n",
            "Epoch 207/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6680 - accuracy: 0.5639\n",
            "Epoch 208/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6689 - accuracy: 0.5639\n",
            "Epoch 209/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6662 - accuracy: 0.5648\n",
            "Epoch 210/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6677 - accuracy: 0.5642\n",
            "Epoch 211/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6669 - accuracy: 0.5643\n",
            "Epoch 212/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6672 - accuracy: 0.5645\n",
            "Epoch 213/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6674 - accuracy: 0.5634\n",
            "Epoch 214/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6707 - accuracy: 0.5628\n",
            "Epoch 215/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6670 - accuracy: 0.5648\n",
            "Epoch 216/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6665 - accuracy: 0.5653\n",
            "Epoch 217/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6671 - accuracy: 0.5654\n",
            "Epoch 218/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6653 - accuracy: 0.5656\n",
            "Epoch 219/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6668 - accuracy: 0.5649\n",
            "Epoch 220/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6655 - accuracy: 0.5654\n",
            "Epoch 221/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6648 - accuracy: 0.5662\n",
            "Epoch 222/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6646 - accuracy: 0.5663\n",
            "Epoch 223/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6658 - accuracy: 0.5662\n",
            "Epoch 224/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6674 - accuracy: 0.5647\n",
            "Epoch 225/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6687 - accuracy: 0.5645\n",
            "Epoch 226/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6655 - accuracy: 0.5666\n",
            "Epoch 227/300\n",
            "875/875 [==============================] - 20s 23ms/step - loss: 0.6644 - accuracy: 0.5673\n",
            "Epoch 228/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6645 - accuracy: 0.5669\n",
            "Epoch 229/300\n",
            "875/875 [==============================] - 21s 23ms/step - loss: 0.6640 - accuracy: 0.5677\n",
            "Epoch 230/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6669 - accuracy: 0.5662\n",
            "Epoch 231/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6703 - accuracy: 0.5625\n",
            "Epoch 232/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6732 - accuracy: 0.5623\n",
            "Epoch 233/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6667 - accuracy: 0.5662\n",
            "Epoch 234/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6643 - accuracy: 0.5677\n",
            "Epoch 235/300\n",
            "875/875 [==============================] - 22s 26ms/step - loss: 0.6638 - accuracy: 0.5681\n",
            "Epoch 236/300\n",
            "875/875 [==============================] - 24s 27ms/step - loss: 0.6657 - accuracy: 0.5669\n",
            "Epoch 237/300\n",
            "875/875 [==============================] - 24s 27ms/step - loss: 0.6649 - accuracy: 0.5668\n",
            "Epoch 238/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6640 - accuracy: 0.5686\n",
            "Epoch 239/300\n",
            "875/875 [==============================] - 23s 26ms/step - loss: 0.6656 - accuracy: 0.5679\n",
            "Epoch 240/300\n",
            "875/875 [==============================] - 23s 26ms/step - loss: 0.6619 - accuracy: 0.5689\n",
            "Epoch 241/300\n",
            "875/875 [==============================] - 23s 26ms/step - loss: 0.6633 - accuracy: 0.5685\n",
            "Epoch 242/300\n",
            "875/875 [==============================] - 23s 26ms/step - loss: 0.6624 - accuracy: 0.5683\n",
            "Epoch 243/300\n",
            "875/875 [==============================] - 23s 27ms/step - loss: 0.6609 - accuracy: 0.5696\n",
            "Epoch 244/300\n",
            "875/875 [==============================] - 22s 26ms/step - loss: 0.6619 - accuracy: 0.5687\n",
            "Epoch 245/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6622 - accuracy: 0.5692\n",
            "Epoch 246/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6628 - accuracy: 0.5687\n",
            "Epoch 247/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6622 - accuracy: 0.5683\n",
            "Epoch 248/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6606 - accuracy: 0.5699\n",
            "Epoch 249/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6617 - accuracy: 0.5683\n",
            "Epoch 250/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6600 - accuracy: 0.5700\n",
            "Epoch 251/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6613 - accuracy: 0.5702\n",
            "Epoch 252/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6668 - accuracy: 0.5667\n",
            "Epoch 253/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6610 - accuracy: 0.5701\n",
            "Epoch 254/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6586 - accuracy: 0.5713\n",
            "Epoch 255/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6596 - accuracy: 0.5706\n",
            "Epoch 256/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6599 - accuracy: 0.5704\n",
            "Epoch 257/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6587 - accuracy: 0.5719\n",
            "Epoch 258/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6580 - accuracy: 0.5707\n",
            "Epoch 259/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6589 - accuracy: 0.5712\n",
            "Epoch 260/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6597 - accuracy: 0.5702\n",
            "Epoch 261/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6588 - accuracy: 0.5717\n",
            "Epoch 262/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6578 - accuracy: 0.5716\n",
            "Epoch 263/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6581 - accuracy: 0.5731\n",
            "Epoch 264/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6577 - accuracy: 0.5722\n",
            "Epoch 265/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6568 - accuracy: 0.5717\n",
            "Epoch 266/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6564 - accuracy: 0.5729\n",
            "Epoch 267/300\n",
            "875/875 [==============================] - 21s 25ms/step - loss: 0.6564 - accuracy: 0.5730\n",
            "Epoch 268/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6575 - accuracy: 0.5722\n",
            "Epoch 269/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6554 - accuracy: 0.5734\n",
            "Epoch 270/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6574 - accuracy: 0.5724\n",
            "Epoch 271/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6562 - accuracy: 0.5731\n",
            "Epoch 272/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6551 - accuracy: 0.5739\n",
            "Epoch 273/300\n",
            "875/875 [==============================] - 21s 25ms/step - loss: 0.6558 - accuracy: 0.5741\n",
            "Epoch 274/300\n",
            "875/875 [==============================] - 21s 25ms/step - loss: 0.6573 - accuracy: 0.5733\n",
            "Epoch 275/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6545 - accuracy: 0.5739\n",
            "Epoch 276/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6552 - accuracy: 0.5739\n",
            "Epoch 277/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6619 - accuracy: 0.5706\n",
            "Epoch 278/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6533 - accuracy: 0.5752\n",
            "Epoch 279/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6531 - accuracy: 0.5759\n",
            "Epoch 280/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6542 - accuracy: 0.5744\n",
            "Epoch 281/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6539 - accuracy: 0.5760\n",
            "Epoch 282/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6527 - accuracy: 0.5767\n",
            "Epoch 283/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6550 - accuracy: 0.5750\n",
            "Epoch 284/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6564 - accuracy: 0.5744\n",
            "Epoch 285/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6540 - accuracy: 0.5766\n",
            "Epoch 286/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6537 - accuracy: 0.5766\n",
            "Epoch 287/300\n",
            "875/875 [==============================] - 21s 25ms/step - loss: 0.6556 - accuracy: 0.5744\n",
            "Epoch 288/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6520 - accuracy: 0.5765\n",
            "Epoch 289/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6498 - accuracy: 0.5773\n",
            "Epoch 290/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6552 - accuracy: 0.5757\n",
            "Epoch 291/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6549 - accuracy: 0.5756\n",
            "Epoch 292/300\n",
            "875/875 [==============================] - 21s 24ms/step - loss: 0.6599 - accuracy: 0.5762\n",
            "Epoch 293/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6525 - accuracy: 0.5773\n",
            "Epoch 294/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6519 - accuracy: 0.5768\n",
            "Epoch 295/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6521 - accuracy: 0.5776\n",
            "Epoch 296/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6508 - accuracy: 0.5790\n",
            "Epoch 297/300\n",
            "875/875 [==============================] - 23s 26ms/step - loss: 0.6522 - accuracy: 0.5778\n",
            "Epoch 298/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6512 - accuracy: 0.5777\n",
            "Epoch 299/300\n",
            "875/875 [==============================] - 23s 26ms/step - loss: 0.6560 - accuracy: 0.5773\n",
            "Epoch 300/300\n",
            "875/875 [==============================] - 22s 25ms/step - loss: 0.6506 - accuracy: 0.5784\n",
            "Accuracy: 53.47%\n",
            "[4437]\n",
            "(24000, 1)\n",
            "(24000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STXsvJH4zcKs",
        "outputId": "3e2b6b92-8382-42eb-e7d6-1de20d9bfebf"
      },
      "source": [
        "model.save('model_300ep_5feature30min')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model_300ep_5feature30min/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: model_300ep_5feature30min/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f5e12c62c90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f5e0d831350> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4AmEMHKzInL"
      },
      "source": [
        "## results 1 network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4EMafUfbVMb"
      },
      "source": [
        "### proportion of ups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hU8kJqI0QRD",
        "outputId": "80be49b8-4814-443e-a319-73a880e7c946"
      },
      "source": [
        "sum(y_test) / 24000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4535416666666667"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA7ysuzI1AsE"
      },
      "source": [
        "prediction = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozJkocHD1aON",
        "outputId": "4bc0752e-c6e8-4ecd-9e24-478e44c2ce0f"
      },
      "source": [
        "prediction.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBumaknN1F5x",
        "outputId": "fb18e684-1240-4bba-cf6e-1aab7b05c304"
      },
      "source": [
        "where1 = np.where(prediction < 0.5)[0]\n",
        "print(where1.shape)\n",
        "print(sum(y_test[where1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19051,)\n",
            "8578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYtfwxXX1Tm5",
        "outputId": "9a28f606-bfe4-4b29-c2d1-38be664be828"
      },
      "source": [
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "sum((y_pred.reshape(y_pred.shape[0]) == y_test).astype(int)) / 24000 #prediction quality full"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.53475"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgj6TC8ObOtW"
      },
      "source": [
        "### correct predictions - up\n",
        "higher then proportion of up in data, which is 0.4535\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVgzkkQszdqh",
        "outputId": "605ad1bb-633c-4437-bd4f-c4f2870fb873"
      },
      "source": [
        "print(sum(y_test) / 24000)\n",
        "\n",
        "prediction = model.predict(X_test)\n",
        "where1 = np.where(prediction >= 0.50)[0]\n",
        "print(where1.shape)\n",
        "print(sum(y_test[where1]) / where1.shape) #quality up, \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4535416666666667\n",
            "(4437,)\n",
            "[0.46833446]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZuaH6btbg9O"
      },
      "source": [
        "### correct predictions - down\n",
        "higher then proportion of down in data, which is 0.5465"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEUu2kK0zdne",
        "outputId": "37ab2fc4-1f4e-4230-d689-8c7afeff11df"
      },
      "source": [
        "print(sum(y_test) / 24000)\n",
        "\n",
        "prediction = model.predict(X_test)\n",
        "where1 = np.where(prediction <= 0.50)[0]\n",
        "print(where1.shape)\n",
        "print((where1.shape[0] - sum(y_test[where1]) )/ where1.shape) #quality down, higher then proportion of down in data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4535416666666667\n",
            "(19563,)\n",
            "[0.54981342]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq_KVYt06VKR"
      },
      "source": [
        "## second try\n",
        "~40 days training, ~15 testing <br>\n",
        "60 previous minutes <br>\n",
        "5 features - open, close, volume, high, low <br>\n",
        "now predicts 15 minutes into the future - will go up or noy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f18amVnk1f7E",
        "outputId": "764e683a-ebd6-4c4c-836c-051ca2c8508d"
      },
      "source": [
        "X, Y = process_prices(kline_df, time_window=15, period_len=60)\n",
        "\n",
        "X3 = add_feature(X, 'high', period_len=60, kline_df=kline_df)\n",
        "X3 = add_feature(X3, 'low', period_len=60, kline_df=kline_df)\n",
        "X3 = add_feature(X3, 'close', period_len=60, kline_df=kline_df)\n",
        "X3 = add_feature(X3, 'vol', period_len=60, kline_df=kline_df)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(425972, 60)\n",
            "(425972,)\n",
            "(426413, 60)\n",
            "(426413, 60)\n",
            "(426413, 60)\n",
            "(426413, 60)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN3S4HscX3Jg",
        "outputId": "67c29864-1732-4fa7-ec9a-23340dd201a1"
      },
      "source": [
        "model2, X_train2, X_test2, y_train2, y_test2 = test_strategy_long_nn(X3[-80000:], Y[-80000:], test_size = 0.3, train_size=0.7)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(56000, 60, 5)\n",
            "fraction is 0.4961607142857143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_4 (LSTM)               (None, 60, 50)            11200     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 60, 50)            0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 50)                20200     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,451\n",
            "Trainable params: 31,451\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "(56000, 60, 5)\n",
            "(24000, 60, 5)\n",
            "(56000,)\n",
            "(24000,)\n",
            "Epoch 1/300\n",
            "875/875 [==============================] - 41s 44ms/step - loss: 0.6933 - accuracy: 0.5031\n",
            "Epoch 2/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6932 - accuracy: 0.5022\n",
            "Epoch 3/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6932 - accuracy: 0.5013\n",
            "Epoch 4/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6931 - accuracy: 0.5027\n",
            "Epoch 5/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6931 - accuracy: 0.5050\n",
            "Epoch 6/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6930 - accuracy: 0.5048\n",
            "Epoch 7/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6930 - accuracy: 0.5037\n",
            "Epoch 8/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6929 - accuracy: 0.5046\n",
            "Epoch 9/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6929 - accuracy: 0.5049\n",
            "Epoch 10/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6930 - accuracy: 0.5055\n",
            "Epoch 11/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6930 - accuracy: 0.5063\n",
            "Epoch 12/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6930 - accuracy: 0.5045\n",
            "Epoch 13/300\n",
            "875/875 [==============================] - 37s 43ms/step - loss: 0.6929 - accuracy: 0.5061\n",
            "Epoch 14/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6929 - accuracy: 0.5043\n",
            "Epoch 15/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6929 - accuracy: 0.5053\n",
            "Epoch 16/300\n",
            "875/875 [==============================] - 37s 43ms/step - loss: 0.6929 - accuracy: 0.5044\n",
            "Epoch 17/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6928 - accuracy: 0.5057\n",
            "Epoch 18/300\n",
            "875/875 [==============================] - 37s 43ms/step - loss: 0.6939 - accuracy: 0.5041\n",
            "Epoch 19/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6936 - accuracy: 0.4977\n",
            "Epoch 20/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6936 - accuracy: 0.5058\n",
            "Epoch 21/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6934 - accuracy: 0.5023\n",
            "Epoch 22/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6936 - accuracy: 0.5021\n",
            "Epoch 23/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6934 - accuracy: 0.5026\n",
            "Epoch 24/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6935 - accuracy: 0.4982\n",
            "Epoch 25/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6935 - accuracy: 0.5014\n",
            "Epoch 26/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6934 - accuracy: 0.5036\n",
            "Epoch 27/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6935 - accuracy: 0.4972\n",
            "Epoch 28/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6936 - accuracy: 0.4983\n",
            "Epoch 29/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6935 - accuracy: 0.5001\n",
            "Epoch 30/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6935 - accuracy: 0.4997\n",
            "Epoch 31/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6935 - accuracy: 0.4987\n",
            "Epoch 32/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6935 - accuracy: 0.4984\n",
            "Epoch 33/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6934 - accuracy: 0.5016\n",
            "Epoch 34/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6935 - accuracy: 0.5041\n",
            "Epoch 35/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6933 - accuracy: 0.5037\n",
            "Epoch 36/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6935 - accuracy: 0.5009\n",
            "Epoch 37/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6935 - accuracy: 0.4996\n",
            "Epoch 38/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6934 - accuracy: 0.5056\n",
            "Epoch 39/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6936 - accuracy: 0.4986\n",
            "Epoch 40/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6934 - accuracy: 0.5027\n",
            "Epoch 41/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6935 - accuracy: 0.5010\n",
            "Epoch 42/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6935 - accuracy: 0.5022\n",
            "Epoch 43/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6935 - accuracy: 0.5019\n",
            "Epoch 44/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6935 - accuracy: 0.5025\n",
            "Epoch 45/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6935 - accuracy: 0.4991\n",
            "Epoch 46/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6935 - accuracy: 0.5016\n",
            "Epoch 47/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6935 - accuracy: 0.5010\n",
            "Epoch 48/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6934 - accuracy: 0.5044\n",
            "Epoch 49/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6937 - accuracy: 0.4991\n",
            "Epoch 50/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6935 - accuracy: 0.5007\n",
            "Epoch 51/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6935 - accuracy: 0.5015\n",
            "Epoch 52/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6935 - accuracy: 0.4972\n",
            "Epoch 53/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6934 - accuracy: 0.4986\n",
            "Epoch 54/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6933 - accuracy: 0.5022\n",
            "Epoch 55/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6935 - accuracy: 0.5002\n",
            "Epoch 56/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6934 - accuracy: 0.5005\n",
            "Epoch 57/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6935 - accuracy: 0.5008\n",
            "Epoch 58/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6935 - accuracy: 0.4992\n",
            "Epoch 59/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6944 - accuracy: 0.4944\n",
            "Epoch 60/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6934 - accuracy: 0.5005\n",
            "Epoch 61/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6935 - accuracy: 0.5022\n",
            "Epoch 62/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6934 - accuracy: 0.5023\n",
            "Epoch 63/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6935 - accuracy: 0.5008\n",
            "Epoch 64/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6934 - accuracy: 0.5019\n",
            "Epoch 65/300\n",
            "875/875 [==============================] - 47s 54ms/step - loss: 0.6934 - accuracy: 0.5024\n",
            "Epoch 66/300\n",
            "875/875 [==============================] - 45s 52ms/step - loss: 0.6935 - accuracy: 0.5013\n",
            "Epoch 67/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6935 - accuracy: 0.5013\n",
            "Epoch 68/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6935 - accuracy: 0.5024\n",
            "Epoch 69/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6934 - accuracy: 0.4987\n",
            "Epoch 70/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6934 - accuracy: 0.5005\n",
            "Epoch 71/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6934 - accuracy: 0.5019\n",
            "Epoch 72/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6934 - accuracy: 0.5037\n",
            "Epoch 73/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6934 - accuracy: 0.5038\n",
            "Epoch 74/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6934 - accuracy: 0.5014\n",
            "Epoch 75/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6933 - accuracy: 0.5024\n",
            "Epoch 76/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6934 - accuracy: 0.5013\n",
            "Epoch 77/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6934 - accuracy: 0.5036\n",
            "Epoch 78/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6934 - accuracy: 0.4992\n",
            "Epoch 79/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6934 - accuracy: 0.5006\n",
            "Epoch 80/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6934 - accuracy: 0.5019\n",
            "Epoch 81/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6934 - accuracy: 0.5017\n",
            "Epoch 82/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6934 - accuracy: 0.5022\n",
            "Epoch 83/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6935 - accuracy: 0.5001\n",
            "Epoch 84/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6934 - accuracy: 0.5037\n",
            "Epoch 85/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6935 - accuracy: 0.5006\n",
            "Epoch 86/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6934 - accuracy: 0.5005\n",
            "Epoch 87/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6934 - accuracy: 0.5009\n",
            "Epoch 88/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6934 - accuracy: 0.5004\n",
            "Epoch 89/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6934 - accuracy: 0.5008\n",
            "Epoch 90/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6935 - accuracy: 0.5016\n",
            "Epoch 91/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6933 - accuracy: 0.5031\n",
            "Epoch 92/300\n",
            "875/875 [==============================] - 38s 43ms/step - loss: 0.6935 - accuracy: 0.5012\n",
            "Epoch 93/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6934 - accuracy: 0.4988\n",
            "Epoch 94/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6935 - accuracy: 0.4978\n",
            "Epoch 95/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6934 - accuracy: 0.4991\n",
            "Epoch 96/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6935 - accuracy: 0.5010\n",
            "Epoch 97/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6934 - accuracy: 0.4979\n",
            "Epoch 98/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6934 - accuracy: 0.5042\n",
            "Epoch 99/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6934 - accuracy: 0.5028\n",
            "Epoch 100/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6934 - accuracy: 0.5024\n",
            "Epoch 101/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6935 - accuracy: 0.5026\n",
            "Epoch 102/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6932 - accuracy: 0.5035\n",
            "Epoch 103/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6933 - accuracy: 0.5058\n",
            "Epoch 104/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6934 - accuracy: 0.5015\n",
            "Epoch 105/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6934 - accuracy: 0.5029\n",
            "Epoch 106/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6934 - accuracy: 0.5053\n",
            "Epoch 107/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6935 - accuracy: 0.5000\n",
            "Epoch 108/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6932 - accuracy: 0.5033\n",
            "Epoch 109/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6934 - accuracy: 0.5040\n",
            "Epoch 110/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6935 - accuracy: 0.5001\n",
            "Epoch 111/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6933 - accuracy: 0.5022\n",
            "Epoch 112/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6933 - accuracy: 0.5054\n",
            "Epoch 113/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6934 - accuracy: 0.5003\n",
            "Epoch 114/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6932 - accuracy: 0.5085\n",
            "Epoch 115/300\n",
            "875/875 [==============================] - 38s 44ms/step - loss: 0.6934 - accuracy: 0.5011\n",
            "Epoch 116/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6932 - accuracy: 0.5043\n",
            "Epoch 117/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6932 - accuracy: 0.5047\n",
            "Epoch 118/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6934 - accuracy: 0.5003\n",
            "Epoch 119/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6932 - accuracy: 0.5029\n",
            "Epoch 120/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6932 - accuracy: 0.4993\n",
            "Epoch 121/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5049\n",
            "Epoch 122/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5041\n",
            "Epoch 123/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6932 - accuracy: 0.5047\n",
            "Epoch 124/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6935 - accuracy: 0.5011\n",
            "Epoch 125/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6932 - accuracy: 0.5070\n",
            "Epoch 126/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6934 - accuracy: 0.5038\n",
            "Epoch 127/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6934 - accuracy: 0.5041\n",
            "Epoch 128/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6933 - accuracy: 0.5022\n",
            "Epoch 129/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6934 - accuracy: 0.5024\n",
            "Epoch 130/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6933 - accuracy: 0.5039\n",
            "Epoch 131/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6933 - accuracy: 0.5029\n",
            "Epoch 132/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6932 - accuracy: 0.5014\n",
            "Epoch 133/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6934 - accuracy: 0.5040\n",
            "Epoch 134/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6933 - accuracy: 0.5034\n",
            "Epoch 135/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6934 - accuracy: 0.5025\n",
            "Epoch 136/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6933 - accuracy: 0.5034\n",
            "Epoch 137/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6933 - accuracy: 0.5041\n",
            "Epoch 138/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6933 - accuracy: 0.5018\n",
            "Epoch 139/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6933 - accuracy: 0.4985\n",
            "Epoch 140/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6933 - accuracy: 0.5020\n",
            "Epoch 141/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6932 - accuracy: 0.5020\n",
            "Epoch 142/300\n",
            "875/875 [==============================] - 44s 50ms/step - loss: 0.6934 - accuracy: 0.5033\n",
            "Epoch 143/300\n",
            "875/875 [==============================] - 47s 54ms/step - loss: 0.6933 - accuracy: 0.5035\n",
            "Epoch 144/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6932 - accuracy: 0.5007\n",
            "Epoch 145/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6933 - accuracy: 0.5031\n",
            "Epoch 146/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6932 - accuracy: 0.5034\n",
            "Epoch 147/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6932 - accuracy: 0.5037\n",
            "Epoch 148/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6933 - accuracy: 0.5021\n",
            "Epoch 149/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6931 - accuracy: 0.5052\n",
            "Epoch 150/300\n",
            "875/875 [==============================] - 44s 51ms/step - loss: 0.6931 - accuracy: 0.5076\n",
            "Epoch 151/300\n",
            "875/875 [==============================] - 44s 50ms/step - loss: 0.6934 - accuracy: 0.5018\n",
            "Epoch 152/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6931 - accuracy: 0.5038\n",
            "Epoch 153/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6933 - accuracy: 0.5023\n",
            "Epoch 154/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6931 - accuracy: 0.5030\n",
            "Epoch 155/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6932 - accuracy: 0.5006\n",
            "Epoch 156/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6931 - accuracy: 0.5059\n",
            "Epoch 157/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6931 - accuracy: 0.5026\n",
            "Epoch 158/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6933 - accuracy: 0.5015\n",
            "Epoch 159/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6931 - accuracy: 0.5036\n",
            "Epoch 160/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6933 - accuracy: 0.5039\n",
            "Epoch 161/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6931 - accuracy: 0.5063\n",
            "Epoch 162/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6932 - accuracy: 0.5039\n",
            "Epoch 163/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6933 - accuracy: 0.5053\n",
            "Epoch 164/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6933 - accuracy: 0.5005\n",
            "Epoch 165/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6931 - accuracy: 0.5038\n",
            "Epoch 166/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6932 - accuracy: 0.5036\n",
            "Epoch 167/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6931 - accuracy: 0.5043\n",
            "Epoch 168/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5024\n",
            "Epoch 169/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6931 - accuracy: 0.5027\n",
            "Epoch 170/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6931 - accuracy: 0.5031\n",
            "Epoch 171/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6931 - accuracy: 0.4976\n",
            "Epoch 172/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6931 - accuracy: 0.5027\n",
            "Epoch 173/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6931 - accuracy: 0.5013\n",
            "Epoch 174/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6930 - accuracy: 0.5050\n",
            "Epoch 175/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5011\n",
            "Epoch 176/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6932 - accuracy: 0.5032\n",
            "Epoch 177/300\n",
            "875/875 [==============================] - 42s 49ms/step - loss: 0.6932 - accuracy: 0.5034\n",
            "Epoch 178/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6932 - accuracy: 0.5017\n",
            "Epoch 179/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6932 - accuracy: 0.5037\n",
            "Epoch 180/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6933 - accuracy: 0.5034\n",
            "Epoch 181/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6933 - accuracy: 0.4997\n",
            "Epoch 182/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6933 - accuracy: 0.4999\n",
            "Epoch 183/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6933 - accuracy: 0.5038\n",
            "Epoch 184/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6932 - accuracy: 0.5041\n",
            "Epoch 185/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6931 - accuracy: 0.5056\n",
            "Epoch 186/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6931 - accuracy: 0.5038\n",
            "Epoch 187/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6931 - accuracy: 0.4994\n",
            "Epoch 188/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6933 - accuracy: 0.5019\n",
            "Epoch 189/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6933 - accuracy: 0.5002\n",
            "Epoch 190/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5023\n",
            "Epoch 191/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6931 - accuracy: 0.5036\n",
            "Epoch 192/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6931 - accuracy: 0.5042\n",
            "Epoch 193/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6930 - accuracy: 0.5029\n",
            "Epoch 194/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6930 - accuracy: 0.5022\n",
            "Epoch 195/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6930 - accuracy: 0.5015\n",
            "Epoch 196/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6930 - accuracy: 0.5007\n",
            "Epoch 197/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6931 - accuracy: 0.5033\n",
            "Epoch 198/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6930 - accuracy: 0.5022\n",
            "Epoch 199/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6933 - accuracy: 0.5051\n",
            "Epoch 200/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6932 - accuracy: 0.5019\n",
            "Epoch 201/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6930 - accuracy: 0.5024\n",
            "Epoch 202/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6931 - accuracy: 0.5017\n",
            "Epoch 203/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6930 - accuracy: 0.5012\n",
            "Epoch 204/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6934 - accuracy: 0.5025\n",
            "Epoch 205/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6933 - accuracy: 0.5032\n",
            "Epoch 206/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6934 - accuracy: 0.5024\n",
            "Epoch 207/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5017\n",
            "Epoch 208/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5035\n",
            "Epoch 209/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6932 - accuracy: 0.5060\n",
            "Epoch 210/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6933 - accuracy: 0.5029\n",
            "Epoch 211/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6932 - accuracy: 0.5049\n",
            "Epoch 212/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5026\n",
            "Epoch 213/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5034\n",
            "Epoch 214/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6933 - accuracy: 0.5049\n",
            "Epoch 215/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5036\n",
            "Epoch 216/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6933 - accuracy: 0.5031\n",
            "Epoch 217/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6933 - accuracy: 0.5013\n",
            "Epoch 218/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5035\n",
            "Epoch 219/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5047\n",
            "Epoch 220/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5034\n",
            "Epoch 221/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6932 - accuracy: 0.5045\n",
            "Epoch 222/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6933 - accuracy: 0.5026\n",
            "Epoch 223/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6933 - accuracy: 0.5011\n",
            "Epoch 224/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6933 - accuracy: 0.5017\n",
            "Epoch 225/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6932 - accuracy: 0.5028\n",
            "Epoch 226/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6933 - accuracy: 0.5054\n",
            "Epoch 227/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6933 - accuracy: 0.5018\n",
            "Epoch 228/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6932 - accuracy: 0.5057\n",
            "Epoch 229/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6933 - accuracy: 0.5046\n",
            "Epoch 230/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6932 - accuracy: 0.5046\n",
            "Epoch 231/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6933 - accuracy: 0.5008\n",
            "Epoch 232/300\n",
            "875/875 [==============================] - 44s 50ms/step - loss: 0.6932 - accuracy: 0.5011\n",
            "Epoch 233/300\n",
            "875/875 [==============================] - 44s 50ms/step - loss: 0.6932 - accuracy: 0.5054\n",
            "Epoch 234/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6933 - accuracy: 0.5006\n",
            "Epoch 235/300\n",
            "875/875 [==============================] - 45s 52ms/step - loss: 0.6931 - accuracy: 0.5064\n",
            "Epoch 236/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6932 - accuracy: 0.5029\n",
            "Epoch 237/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6933 - accuracy: 0.5033\n",
            "Epoch 238/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6933 - accuracy: 0.5031\n",
            "Epoch 239/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6933 - accuracy: 0.5009\n",
            "Epoch 240/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6933 - accuracy: 0.5009\n",
            "Epoch 241/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6932 - accuracy: 0.5063\n",
            "Epoch 242/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6931 - accuracy: 0.5083\n",
            "Epoch 243/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6932 - accuracy: 0.5040\n",
            "Epoch 244/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6931 - accuracy: 0.5062\n",
            "Epoch 245/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6931 - accuracy: 0.5059\n",
            "Epoch 246/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6932 - accuracy: 0.5053\n",
            "Epoch 247/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6930 - accuracy: 0.5063\n",
            "Epoch 248/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6929 - accuracy: 0.5075\n",
            "Epoch 249/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6933 - accuracy: 0.5006\n",
            "Epoch 250/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6932 - accuracy: 0.5043\n",
            "Epoch 251/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6933 - accuracy: 0.5034\n",
            "Epoch 252/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6933 - accuracy: 0.5028\n",
            "Epoch 253/300\n",
            "875/875 [==============================] - 42s 47ms/step - loss: 0.6932 - accuracy: 0.5031\n",
            "Epoch 254/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6932 - accuracy: 0.5064\n",
            "Epoch 255/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6931 - accuracy: 0.5058\n",
            "Epoch 256/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6932 - accuracy: 0.5037\n",
            "Epoch 257/300\n",
            "875/875 [==============================] - 40s 45ms/step - loss: 0.6934 - accuracy: 0.5051\n",
            "Epoch 258/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5056\n",
            "Epoch 259/300\n",
            "875/875 [==============================] - 39s 45ms/step - loss: 0.6932 - accuracy: 0.5031\n",
            "Epoch 260/300\n",
            "875/875 [==============================] - 39s 44ms/step - loss: 0.6932 - accuracy: 0.5066\n",
            "Epoch 261/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5012\n",
            "Epoch 262/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6932 - accuracy: 0.5048\n",
            "Epoch 263/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5032\n",
            "Epoch 264/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6931 - accuracy: 0.5061\n",
            "Epoch 265/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6931 - accuracy: 0.5031\n",
            "Epoch 266/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6931 - accuracy: 0.5068\n",
            "Epoch 267/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6930 - accuracy: 0.5103\n",
            "Epoch 268/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5036\n",
            "Epoch 269/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6931 - accuracy: 0.5071\n",
            "Epoch 270/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6933 - accuracy: 0.5025\n",
            "Epoch 271/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6932 - accuracy: 0.5022\n",
            "Epoch 272/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6932 - accuracy: 0.5047\n",
            "Epoch 273/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6932 - accuracy: 0.5033\n",
            "Epoch 274/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6930 - accuracy: 0.5053\n",
            "Epoch 275/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6930 - accuracy: 0.5057\n",
            "Epoch 276/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6930 - accuracy: 0.5066\n",
            "Epoch 277/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6933 - accuracy: 0.5009\n",
            "Epoch 278/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6932 - accuracy: 0.5013\n",
            "Epoch 279/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6931 - accuracy: 0.5061\n",
            "Epoch 280/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6932 - accuracy: 0.5001\n",
            "Epoch 281/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6931 - accuracy: 0.5059\n",
            "Epoch 282/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6932 - accuracy: 0.5016\n",
            "Epoch 283/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6930 - accuracy: 0.5084\n",
            "Epoch 284/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6931 - accuracy: 0.5040\n",
            "Epoch 285/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6931 - accuracy: 0.5048\n",
            "Epoch 286/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6931 - accuracy: 0.5056\n",
            "Epoch 287/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6931 - accuracy: 0.5071\n",
            "Epoch 288/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6930 - accuracy: 0.5054\n",
            "Epoch 289/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6930 - accuracy: 0.5080\n",
            "Epoch 290/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6930 - accuracy: 0.5050\n",
            "Epoch 291/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6931 - accuracy: 0.5060\n",
            "Epoch 292/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6930 - accuracy: 0.5067\n",
            "Epoch 293/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6931 - accuracy: 0.5041\n",
            "Epoch 294/300\n",
            "875/875 [==============================] - 43s 49ms/step - loss: 0.6931 - accuracy: 0.5037\n",
            "Epoch 295/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6932 - accuracy: 0.5035\n",
            "Epoch 296/300\n",
            "875/875 [==============================] - 41s 47ms/step - loss: 0.6931 - accuracy: 0.5055\n",
            "Epoch 297/300\n",
            "875/875 [==============================] - 42s 48ms/step - loss: 0.6931 - accuracy: 0.5051\n",
            "Epoch 298/300\n",
            "875/875 [==============================] - 41s 46ms/step - loss: 0.6930 - accuracy: 0.5069\n",
            "Epoch 299/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6931 - accuracy: 0.5077\n",
            "Epoch 300/300\n",
            "875/875 [==============================] - 40s 46ms/step - loss: 0.6930 - accuracy: 0.5074\n",
            "Accuracy: 51.40%\n",
            "[4949]\n",
            "(24000, 1)\n",
            "(24000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BPJDfiaX3G6",
        "outputId": "d5952b75-e1b0-4474-fcb6-f25fea75be6f"
      },
      "source": [
        "model2.save('model_300ep_5feature60min_plus15')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model_300ep_5feature60min_plus15/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: model_300ep_5feature60min_plus15/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f5e040aaa90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f5e040dc210> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yof2YSozvzuF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI6lJ89Qv273"
      },
      "source": [
        "## resuts second network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNWKyFTvX3Dt",
        "outputId": "d2312a1f-86f4-47bb-acf1-6451be8bb69d"
      },
      "source": [
        "print(sum(y_test2) / 24000) # proportion"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaz_boPuOL7-"
      },
      "source": [
        "total accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZsXoDjVtE5L",
        "outputId": "7799d0d8-d58e-44a3-f864-87196fabb26c"
      },
      "source": [
        "y_pred2 = (model2.predict(X_test2) > 0.5).astype(\"int32\")\n",
        "sum((y_pred2.reshape(y_pred2.shape[0]) == y_test2).astype(int)) / 24000 #prediction quality"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5139583333333333"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEJq1dmscB3I"
      },
      "source": [
        "### correct prediction - up\n",
        "much better then random, up from 0.493 proportion in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybtZV0IyuRl-",
        "outputId": "b5e19b2b-676a-4cec-e8dd-3cf1ed60e11e"
      },
      "source": [
        "print(sum(y_test2) / 24000)\n",
        "prediction = model2.predict(X_test2)\n",
        "where1 = np.where(prediction >= 0.50)[0]\n",
        "print(where1.shape)\n",
        "print(sum(y_test2[where1]) / where1.shape) #quality up\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.493\n",
            "(4949,)\n",
            "[0.5168721]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFrXeGjCcHHk"
      },
      "source": [
        "### correct prediction - down\n",
        "also better then random, up from 0.507 proportion in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKDP0f1_wNhY",
        "outputId": "736a7fa7-7c6f-4173-e55e-9adc5baeb9b1"
      },
      "source": [
        "print(sum(y_test2) / 24000)\n",
        "prediction = model2.predict(X_test2)\n",
        "where1 = np.where(prediction <= 0.50)[0]\n",
        "print(where1.shape)\n",
        "print((where1.shape[0] - sum(y_test2[where1]) )/ where1.shape) #quality down\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.493\n",
            "(19051,)\n",
            "[0.51320141]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXuzqv96u76J",
        "outputId": "ec5ea04b-11e2-4811-e142-baca60f7ce57"
      },
      "source": [
        "prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.55799615],\n",
              "       [0.5586262 ],\n",
              "       [0.5592814 ],\n",
              "       ...,\n",
              "       [0.4868262 ],\n",
              "       [0.48685765],\n",
              "       [0.4868237 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-OlvZuqu73Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZmvq9dbu70L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}